# Load Claddis library:#
library(Claddis)#
#
# Set working directory:#
setwd("~/Documents/Homepage/www.graemetlloyd.com")#
#
# Get file list:#
file.list <- list.files()#
#
# Get just the group matrix pages:#
file.list <- file.list[grep("matr[a-z]{4}.html", file.list)]#
#
# Vector for storing output:#
results <- vector(mode = "character")#
#
# Main loop:#
for(i in 1:length(file.list)) {#
  # Read in ith file:#
  X <- scan(file.list[i], what = "", sep = "\n", quiet = TRUE)#
  # Find first p tag opening:#
  begins <- grep("<p class=\"hangingindent\">", X)#
  # FInd last p tag closing:#
  ends <- grep("</p>", X)#
  # Reduce X to just the portion with references:#
  X <- X[begins[1]:ends[length(ends)]]#
  # Find where p tags open:#
  begins <- grep("<p class=\"hangingindent\">", X)#
  # Find where p tags close:#
  ends <- grep("</p>", X)#
  # Check p tags are closed and warn if not:#
  if(length(begins) != length(ends)) print(paste("Error in", file.list[i]))#
  # For each set of p tags:#
  for(j in 1:length(ends)) {#
    # Get full reference block:#
    Y <- X[begins[j]:ends[j]]#
    # Only proceed if this has not already been dealt with:#
    if(length(grep("<a href", Y)) == 0) {#
      # Remove bookmarks:#
      Y <- gsub("</p>", "", gsub("<p class=\"hangingindent\">", "", Y))#
      # Strip out leading whitespace:#
      while(length(grep("\t", Y)) > 0) Y <- gsub("\t", " ", Y)#
      # Strip out leading whitespace:#
      while(length(grep("  ", Y)) > 0) Y <- gsub("  ", " ", Y)#
      # Strip out last leading whitespace"#
      for(k in 1:length(Y)) Y[k] <- paste(strsplit(Y[k], "")[[1]][2:length(strsplit(Y[k], "")[[1]])], collapse = "")#
      # Isolate author and year:#
      authorandyear <- strsplit(gsub(" and ", "%%", gsub("\\., ", ".%%", Y[1])), "%%")[[1]]#
      # Isolate title:#
      title <- Y[2]#
      ##
      locale <- gsub("</b>", "", gsub("<b>", "", gsub("</em>", "", gsub("<em>", "", strsplit(gsub("\\.", "", gsub(", ", "%%", Y[3])), "%%")[[1]]))))#
      ##
      authorline <- paste("\t\t<Author>\n", paste("\t\t\t<List>", authorandyear[1:(length(authorandyear) - 1)], "</List>", sep = "", collapse = "\n"), "\n\t\t</Author>\n", sep = "")#
      ##
      yearline <- paste("\t\t<Year>", gsub("\\.", "", authorandyear[length(authorandyear)]), "</Year>\n", sep = "")#
      ##
      year <- gsub("</Year>\n", "", gsub("\t\t<Year>", "", yearline))#
      ##
      titleline <- strsplit(title, "")[[1]]#
      ##
      if(titleline[length(titleline)] == ".") titleline <- titleline[-length(titleline)]#
      ##
      titleline <- paste(titleline, collapse = "")#
      ##
      titleline <- paste("\t\t<Title>", titleline, "</Title>\n", sep = "")#
      # Case if a book chapter:#
      if(length(grep("In ", locale[1])) == 1) {#
        # Restore locale to original line:#
        locale <- Y[3]#
        ##
        locale <- gsub("<em>In</em> ", "", locale)#
        # Insert first (editor(s)) separator:#
        locale <- gsub(" \\(eds\\.\\) ", "%%", locale)#
        # Insert first (editor(s)) separator:#
        locale <- gsub(" \\(ed\\.\\) ", "%%", locale)#
        # Insert first (editor(s)) separator:#
        locale <- gsub(" \\(eds\\) ", "%%", locale)#
        # Insert first (editor(s)) separator:#
        locale <- gsub(" \\(ed\\) ", "%%", locale)#
        # Isolate editors#
        editors <- strsplit(locale, "%%")[[1]][1]#
        # Add "and" separator:#
        editors <- gsub(" and ", "%%", editors)#
        ##
        if(length(grep(",", editors)) > 0) {#
          # Case if single editor in correct "Surname, Initials" format:#
          if(length(grep("%%", editors)) == 0) editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", editors, "</List>\n", sep = ""), "\t\t</Editor>\n", sep = "")#
          # Case if authors are in incorrect "Intitals Surname" format:#
          if(strsplit(editors, "")[[1]][2] == ".") {#
            # Add separator between names:#
            editors <- gsub(", ", "%%", editors)#
            ##
            editors <- strsplit(editors, "%%")[[1]]#
            ##
            for(k in 1:length(editors)) {#
              ##
              temp <- strsplit(editors[k], "\\. ")[[1]]#
              ##
              editors[k] <- paste(temp[length(temp)], paste(temp[1:(length(temp) - 1)], ".", sep = "", collapse = " "), sep = ", ")#
            }#
            ##
            editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", editors, "</List>\n", sep = "", collapse = ""), "\t\t</Editor>\n", sep = "")#
            ##
          } else {#
            # Add separator between names:#
            editors <- gsub("\\., ", ".%%", editors)#
            ##
            editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", strsplit(editors, "%%")[[1]], "</List>\n", sep = "", collapse = ""), "\t\t</Editor>\n", sep = "")#
          }#
          ##
        } else {#
          # Case if single editor in incorrect "Intitals Surname" format:#
          if(length(grep("%%",editors)) == 0) {#
            ##
            editors <- strsplit(editors, "\\. ")[[1]]#
            ##
            editors <- paste(paste(editors[length(editors)], ",", sep = ""), paste(editors[1:(length(editors) - 1)], ".", sep = "", collapse = " "), collapse = " ")#
            ##
            editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", editors, "</List>\n", sep = ""), "\t\t</Editor>\n", sep = "")#
            # Case of two authors in incorrect "Intitals Surname" format:#
          } else {#
            ##
            editors <- strsplit(editors, "%%")[[1]]#
            ##
            for(k in 1:length(editors)) {#
              ##
              temp <- strsplit(editors[k], "\\. ")[[1]]#
              ##
              editors[k] <- paste(temp[length(temp)], paste(temp[1:(length(temp) - 1)], ".", sep = "", collapse = " "), sep = ", ")#
            }#
            ##
            editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", editors, "</List>\n", sep = "", collapse = ""), "\t\t</Editor>\n", sep = "")#
          }#
        }#
        # Remove editors from rest of book information:#
        locale <- paste(strsplit(locale, "%%")[[1]][2:length(strsplit(locale, "%%")[[1]])], sep = "%%")#
        # Find end of book title separator:#
        locale <- gsub("\\. ", "%%", locale)#
        # Remove trailing period:#
        locale <- gsub("\\.", "", locale)#
        # Isolate booktitle:#
        booktitleline <- paste("\t\t<Booktitle>", strsplit(locale, "%%")[[1]][1], "</Booktitle>\n", sep = "")#
        # Remove booktitle from rest of book information:#
        locale <- paste(strsplit(locale, "%%")[[1]][2:length(strsplit(locale, "%%")[[1]])], sep = "%%")#
        # Remove false gaps:#
        while(length(locale) > 1) locale <- paste(locale, collapse = ". ")#
        # Separate remaining portions:#
        locale <- strsplit(locale, ", ")[[1]]#
        ##
        publisherline <- paste("\t\t<Publisher>", locale[1], "</Publisher>\n", sep = "")#
        ##
        cityline <- paste("\t\t<City>", locale[2], "</City>\n", sep = "")#
        ##
        pagesline <- paste("\t\t<Pages>", gsub("<br>", "", gsub("p", "", locale[3])), "</Pages>\n", sep = "")#
        ##
        fulllines <- paste(authorline, yearline, titleline, "\t\t<Journal/>\n", "\t\t<Volume/>\n", pagesline, booktitleline, publisherline, cityline, editorsline, sep = "")#
        # Case if a journal:#
      } else {#
        ##
        if(year == "in press") {#
          # Case if journal title with commas:#
          if(length(locale) > 2) {#
            # Collapse journal title:#
            locale[1] <- paste(locale[1], locale[2], sep = ", ")#
            # Remove redudnant second part#
            locale <- locale[-2]#
          }#
          # Delete empty volume value#
          if(locale[2] == "") locale <- locale[-2]#
        }#
        # Find journal titles with commas:#
        while(length(locale) > 3) {#
          # Collapse journal title:#
          locale[1] <- paste(locale[1], locale[2], sep = ", ")#
          # Remove redudnant second part:#
          locale <- locale[-2]#
        }#
        ##
        journalline <- paste("\t\t<Journal>", locale[1], "</Journal>\n", sep = "")#
        ##
        if(length(locale) > 1) {#
          ##
          volumeline <- paste("\t\t<Volume>", locale[2], "</Volume>\n", sep = "")#
          ##
        } else {#
          ##
          volumeline <- "\t\t<Volume/>\n"#
        }#
        ##
        if(length(locale) > 2) {#
          ##
          pagesline <- paste("\t\t<Pages>", locale[3], "</Pages>\n", sep = "")#
          ##
        } else {#
          ##
          pagesline <- "\t\t<Pages/>\n"#
        }#
        ##
        fulllines <- paste(authorline, yearline, titleline, journalline, volumeline, pagesline, "\t\t<Booktitle/>\n", "\t\t<Publisher/>\n", "\t\t<City/>\n","\t\t<Editor/>\n", sep = "")#
      }#
    }#
    ##
    results <- c(results, fulllines)#
  }#
}#
#
# Collapse to just unique references (not sure how duplicates ended up in here...):#
results <- sort(unique(results))#
#
# Create empty vector to store hypothetical file names:#
filenames <- vector(mode = "character")#
#
# For each reference:#
for(i in 1:length(results)) {#
  # Isolate authors:#
  authors <- strsplit(strsplit(gsub("\n|\t", "", results[i]), split = "<Author>|</Author>")[[1]][2], split = "<List>|</List>")[[1]][which(nchar(strsplit(strsplit(gsub("\n|\t", "", results[i]), split = "<Author>|</Author>")[[1]][2], split = "<List>|</List>")[[1]]) > 0)]#
  # Isolate surnames:#
  surnames <- unlist(lapply(strsplit(authors, split = ","), '[', 1))#
  # Get publication year:#
  year <- gsub(" ", "", strsplit(gsub("\n|\t", "", results[i]), split = "<Year>|</Year>")[[1]][2])#
  # If a single author:#
  if(length(surnames) == 1) filenames <- c(filenames, gsub("'", "", gsub(" ", "_", paste(surnames, year, sep = "_"))))#
  # If two authors:#
  if(length(surnames) == 2) filenames <- c(filenames, gsub("'", "", gsub(" ", "_", paste(paste(surnames, collapse = "_et_"), year, sep = "_"))))#
  # If more than two authors:#
  if(length(surnames) > 2) filenames <- c(filenames, gsub("'", "", gsub(" ", "_", paste(surnames[1], "etal", year, sep = "_"))))#
}#
#
# Isolate references that have multiple file names (i.e., two or more refrences could be contracted to the same name):#
duplicates <- unique(filenames[duplicated(filenames)])#
#
# Set working directory:#
setwd("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/ToAdd")#
#
# Get list of folders:#
folder.list <- list.files()[-grep("\\.", list.files())]#
#
# Get full paths for each folder:#
for(i in 1:length(folder.list)) folder.list[i] <- paste(getwd(), "/", folder.list[i], sep = "")#
#
# Vector for storing nexus file list:#
file.list <- vector(mode = "character")#
#
# Find all file paths for nexus files:#
for(i in 1:length(folder.list)) {#
  # Set working directory for current folder:#
  setwd(folder.list[i])#
  # Look for NEXUS files:#
  if(length(grep(".nex", list.files())) > 0) {#
    # Add any found to file list:#
    file.list <- c(file.list, paste(folder.list[i], "/", list.files()[grep(".nex", list.files())], sep = ""))#
  }#
}#
#
# Get just the NEXUS file names:#
nexus.files <- unlist(lapply(strsplit(file.list, "/"), '[', 9))#
#
# Reset working directory:#
setwd("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/ToAdd")#
#
# Create vector to store multiple hits:#
multi_hitters <- vector(mode = "character")#
#
# Set scratch counter:#
scratch_counter <- 1#
#
# Create nexus, tnt and xml files:#
for(i in 1:length(file.list)) {#
  # Start feedback:#
  cat("Attempting to read: ", file.list[i], "...")#
  # Get stripped verion of name (i.e., missing a, b, aa etc. ending):#
  stripped_name <- gsub(strsplit(nexus.files[i], "[:0-9:]{4}|inpress")[[1]][2], "", nexus.files[i])#
  # Get hits for stripped name in filenames:#
  hits <- grep(stripped_name, filenames)#
  # Check there is a match:#
  if(length(hits) == 0) stop("No reference with matching name.")#
  # Create reference info:#
  reference_info <- paste(results[hits], collapse = "\n\nOR\n\n")#
  # If multiple hits add to list so these can be manually checked later:#
  if(length(hits) > 1) multi_hitters <- c(multi_hitters, nexus.files[i])#
  # Read in matrix:#
  mymatrix <- ReadMorphNexus(file.list[i])#
  # Update header text:#
  mymatrix$Topper$Header <- "File downloaded from graemetlloyd.com"#
  # Make file name:#
  file.name <- gsub(".nex", "", strsplit(file.list[i], "/")[[1]][length(strsplit(file.list[i], "/")[[1]])])#
  # Write out NEXUS data:#
  #WriteMorphNexus(CladisticMatrix = mymatrix, filename = paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/nexus", "/", file.name, ".nex", sep = ""))#
  # Write out TNT data:#
  #WriteMorphTNT(CladisticMatrix = mymatrix, filename = paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/tnt", "/", file.name, ".tnt", sep = ""))#
  # Write out TNT for analysis:#
  WriteMorphTNT(CladisticMatrix = mymatrix, filename = paste("/Users/eargtl", "/", file.name, ".tnt", sep = ""), add.analysis.block = TRUE)#
}
TNTFA <- readLines(paste("/Users/eargtl", "/", file.name, ".tnt", sep = ""))
grep("scractch,tre", TNTFA, fixed = TRUE)
grep("scratch,tre", TNTFA, fixed = TRUE)
grep("scratch.tre", TNTFA, fixed = TRUE)
length(grep("scratch.tre", TNTFA, fixed = TRUE))
paste(scratch", scratch_counter, ".tre)
paste(scratch", scratch_counter, ".tre")
paste("scratch", scratch_counter, ".tre")
paste("scratch", scratch_counter, ".tre", collapse = "")
paste("scratch", scratch_counter, ".tre", sep = "")
gsub("scratch.tre", paste("scratch", scratch_counter, ".tre", sep = ""), TNTFA, fixed = TRUE)
# Load Claddis library:#
library(Claddis)#
#
# Set working directory:#
setwd("~/Documents/Homepage/www.graemetlloyd.com")#
#
# Get file list:#
file.list <- list.files()#
#
# Get just the group matrix pages:#
file.list <- file.list[grep("matr[a-z]{4}.html", file.list)]#
#
# Vector for storing output:#
results <- vector(mode = "character")#
#
# Main loop:#
for(i in 1:length(file.list)) {#
  # Read in ith file:#
  X <- scan(file.list[i], what = "", sep = "\n", quiet = TRUE)#
  # Find first p tag opening:#
  begins <- grep("<p class=\"hangingindent\">", X)#
  # FInd last p tag closing:#
  ends <- grep("</p>", X)#
  # Reduce X to just the portion with references:#
  X <- X[begins[1]:ends[length(ends)]]#
  # Find where p tags open:#
  begins <- grep("<p class=\"hangingindent\">", X)#
  # Find where p tags close:#
  ends <- grep("</p>", X)#
  # Check p tags are closed and warn if not:#
  if(length(begins) != length(ends)) print(paste("Error in", file.list[i]))#
  # For each set of p tags:#
  for(j in 1:length(ends)) {#
    # Get full reference block:#
    Y <- X[begins[j]:ends[j]]#
    # Only proceed if this has not already been dealt with:#
    if(length(grep("<a href", Y)) == 0) {#
      # Remove bookmarks:#
      Y <- gsub("</p>", "", gsub("<p class=\"hangingindent\">", "", Y))#
      # Strip out leading whitespace:#
      while(length(grep("\t", Y)) > 0) Y <- gsub("\t", " ", Y)#
      # Strip out leading whitespace:#
      while(length(grep("  ", Y)) > 0) Y <- gsub("  ", " ", Y)#
      # Strip out last leading whitespace"#
      for(k in 1:length(Y)) Y[k] <- paste(strsplit(Y[k], "")[[1]][2:length(strsplit(Y[k], "")[[1]])], collapse = "")#
      # Isolate author and year:#
      authorandyear <- strsplit(gsub(" and ", "%%", gsub("\\., ", ".%%", Y[1])), "%%")[[1]]#
      # Isolate title:#
      title <- Y[2]#
      ##
      locale <- gsub("</b>", "", gsub("<b>", "", gsub("</em>", "", gsub("<em>", "", strsplit(gsub("\\.", "", gsub(", ", "%%", Y[3])), "%%")[[1]]))))#
      ##
      authorline <- paste("\t\t<Author>\n", paste("\t\t\t<List>", authorandyear[1:(length(authorandyear) - 1)], "</List>", sep = "", collapse = "\n"), "\n\t\t</Author>\n", sep = "")#
      ##
      yearline <- paste("\t\t<Year>", gsub("\\.", "", authorandyear[length(authorandyear)]), "</Year>\n", sep = "")#
      ##
      year <- gsub("</Year>\n", "", gsub("\t\t<Year>", "", yearline))#
      ##
      titleline <- strsplit(title, "")[[1]]#
      ##
      if(titleline[length(titleline)] == ".") titleline <- titleline[-length(titleline)]#
      ##
      titleline <- paste(titleline, collapse = "")#
      ##
      titleline <- paste("\t\t<Title>", titleline, "</Title>\n", sep = "")#
      # Case if a book chapter:#
      if(length(grep("In ", locale[1])) == 1) {#
        # Restore locale to original line:#
        locale <- Y[3]#
        ##
        locale <- gsub("<em>In</em> ", "", locale)#
        # Insert first (editor(s)) separator:#
        locale <- gsub(" \\(eds\\.\\) ", "%%", locale)#
        # Insert first (editor(s)) separator:#
        locale <- gsub(" \\(ed\\.\\) ", "%%", locale)#
        # Insert first (editor(s)) separator:#
        locale <- gsub(" \\(eds\\) ", "%%", locale)#
        # Insert first (editor(s)) separator:#
        locale <- gsub(" \\(ed\\) ", "%%", locale)#
        # Isolate editors#
        editors <- strsplit(locale, "%%")[[1]][1]#
        # Add "and" separator:#
        editors <- gsub(" and ", "%%", editors)#
        ##
        if(length(grep(",", editors)) > 0) {#
          # Case if single editor in correct "Surname, Initials" format:#
          if(length(grep("%%", editors)) == 0) editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", editors, "</List>\n", sep = ""), "\t\t</Editor>\n", sep = "")#
          # Case if authors are in incorrect "Intitals Surname" format:#
          if(strsplit(editors, "")[[1]][2] == ".") {#
            # Add separator between names:#
            editors <- gsub(", ", "%%", editors)#
            ##
            editors <- strsplit(editors, "%%")[[1]]#
            ##
            for(k in 1:length(editors)) {#
              ##
              temp <- strsplit(editors[k], "\\. ")[[1]]#
              ##
              editors[k] <- paste(temp[length(temp)], paste(temp[1:(length(temp) - 1)], ".", sep = "", collapse = " "), sep = ", ")#
            }#
            ##
            editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", editors, "</List>\n", sep = "", collapse = ""), "\t\t</Editor>\n", sep = "")#
            ##
          } else {#
            # Add separator between names:#
            editors <- gsub("\\., ", ".%%", editors)#
            ##
            editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", strsplit(editors, "%%")[[1]], "</List>\n", sep = "", collapse = ""), "\t\t</Editor>\n", sep = "")#
          }#
          ##
        } else {#
          # Case if single editor in incorrect "Intitals Surname" format:#
          if(length(grep("%%",editors)) == 0) {#
            ##
            editors <- strsplit(editors, "\\. ")[[1]]#
            ##
            editors <- paste(paste(editors[length(editors)], ",", sep = ""), paste(editors[1:(length(editors) - 1)], ".", sep = "", collapse = " "), collapse = " ")#
            ##
            editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", editors, "</List>\n", sep = ""), "\t\t</Editor>\n", sep = "")#
            # Case of two authors in incorrect "Intitals Surname" format:#
          } else {#
            ##
            editors <- strsplit(editors, "%%")[[1]]#
            ##
            for(k in 1:length(editors)) {#
              ##
              temp <- strsplit(editors[k], "\\. ")[[1]]#
              ##
              editors[k] <- paste(temp[length(temp)], paste(temp[1:(length(temp) - 1)], ".", sep = "", collapse = " "), sep = ", ")#
            }#
            ##
            editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", editors, "</List>\n", sep = "", collapse = ""), "\t\t</Editor>\n", sep = "")#
          }#
        }#
        # Remove editors from rest of book information:#
        locale <- paste(strsplit(locale, "%%")[[1]][2:length(strsplit(locale, "%%")[[1]])], sep = "%%")#
        # Find end of book title separator:#
        locale <- gsub("\\. ", "%%", locale)#
        # Remove trailing period:#
        locale <- gsub("\\.", "", locale)#
        # Isolate booktitle:#
        booktitleline <- paste("\t\t<Booktitle>", strsplit(locale, "%%")[[1]][1], "</Booktitle>\n", sep = "")#
        # Remove booktitle from rest of book information:#
        locale <- paste(strsplit(locale, "%%")[[1]][2:length(strsplit(locale, "%%")[[1]])], sep = "%%")#
        # Remove false gaps:#
        while(length(locale) > 1) locale <- paste(locale, collapse = ". ")#
        # Separate remaining portions:#
        locale <- strsplit(locale, ", ")[[1]]#
        ##
        publisherline <- paste("\t\t<Publisher>", locale[1], "</Publisher>\n", sep = "")#
        ##
        cityline <- paste("\t\t<City>", locale[2], "</City>\n", sep = "")#
        ##
        pagesline <- paste("\t\t<Pages>", gsub("<br>", "", gsub("p", "", locale[3])), "</Pages>\n", sep = "")#
        ##
        fulllines <- paste(authorline, yearline, titleline, "\t\t<Journal/>\n", "\t\t<Volume/>\n", pagesline, booktitleline, publisherline, cityline, editorsline, sep = "")#
        # Case if a journal:#
      } else {#
        ##
        if(year == "in press") {#
          # Case if journal title with commas:#
          if(length(locale) > 2) {#
            # Collapse journal title:#
            locale[1] <- paste(locale[1], locale[2], sep = ", ")#
            # Remove redudnant second part#
            locale <- locale[-2]#
          }#
          # Delete empty volume value#
          if(locale[2] == "") locale <- locale[-2]#
        }#
        # Find journal titles with commas:#
        while(length(locale) > 3) {#
          # Collapse journal title:#
          locale[1] <- paste(locale[1], locale[2], sep = ", ")#
          # Remove redudnant second part:#
          locale <- locale[-2]#
        }#
        ##
        journalline <- paste("\t\t<Journal>", locale[1], "</Journal>\n", sep = "")#
        ##
        if(length(locale) > 1) {#
          ##
          volumeline <- paste("\t\t<Volume>", locale[2], "</Volume>\n", sep = "")#
          ##
        } else {#
          ##
          volumeline <- "\t\t<Volume/>\n"#
        }#
        ##
        if(length(locale) > 2) {#
          ##
          pagesline <- paste("\t\t<Pages>", locale[3], "</Pages>\n", sep = "")#
          ##
        } else {#
          ##
          pagesline <- "\t\t<Pages/>\n"#
        }#
        ##
        fulllines <- paste(authorline, yearline, titleline, journalline, volumeline, pagesline, "\t\t<Booktitle/>\n", "\t\t<Publisher/>\n", "\t\t<City/>\n","\t\t<Editor/>\n", sep = "")#
      }#
    }#
    ##
    results <- c(results, fulllines)#
  }#
}#
#
# Collapse to just unique references (not sure how duplicates ended up in here...):#
results <- sort(unique(results))#
#
# Create empty vector to store hypothetical file names:#
filenames <- vector(mode = "character")#
#
# For each reference:#
for(i in 1:length(results)) {#
  # Isolate authors:#
  authors <- strsplit(strsplit(gsub("\n|\t", "", results[i]), split = "<Author>|</Author>")[[1]][2], split = "<List>|</List>")[[1]][which(nchar(strsplit(strsplit(gsub("\n|\t", "", results[i]), split = "<Author>|</Author>")[[1]][2], split = "<List>|</List>")[[1]]) > 0)]#
  # Isolate surnames:#
  surnames <- unlist(lapply(strsplit(authors, split = ","), '[', 1))#
  # Get publication year:#
  year <- gsub(" ", "", strsplit(gsub("\n|\t", "", results[i]), split = "<Year>|</Year>")[[1]][2])#
  # If a single author:#
  if(length(surnames) == 1) filenames <- c(filenames, gsub("'", "", gsub(" ", "_", paste(surnames, year, sep = "_"))))#
  # If two authors:#
  if(length(surnames) == 2) filenames <- c(filenames, gsub("'", "", gsub(" ", "_", paste(paste(surnames, collapse = "_et_"), year, sep = "_"))))#
  # If more than two authors:#
  if(length(surnames) > 2) filenames <- c(filenames, gsub("'", "", gsub(" ", "_", paste(surnames[1], "etal", year, sep = "_"))))#
}#
#
# Isolate references that have multiple file names (i.e., two or more refrences could be contracted to the same name):#
duplicates <- unique(filenames[duplicated(filenames)])#
#
# Set working directory:#
setwd("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/ToAdd")#
#
# Get list of folders:#
folder.list <- list.files()[-grep("\\.", list.files())]#
#
# Get full paths for each folder:#
for(i in 1:length(folder.list)) folder.list[i] <- paste(getwd(), "/", folder.list[i], sep = "")#
#
# Vector for storing nexus file list:#
file.list <- vector(mode = "character")#
#
# Find all file paths for nexus files:#
for(i in 1:length(folder.list)) {#
  # Set working directory for current folder:#
  setwd(folder.list[i])#
  # Look for NEXUS files:#
  if(length(grep(".nex", list.files())) > 0) {#
    # Add any found to file list:#
    file.list <- c(file.list, paste(folder.list[i], "/", list.files()[grep(".nex", list.files())], sep = ""))#
  }#
}#
#
# Get just the NEXUS file names:#
nexus.files <- unlist(lapply(strsplit(file.list, "/"), '[', 9))#
#
# Reset working directory:#
setwd("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/ToAdd")#
#
# Create vector to store multiple hits:#
multi_hitters <- vector(mode = "character")#
#
# Set scratch counter:#
scratch_counter <- 1#
#
# Create nexus, tnt and xml files:#
for(i in 1:length(file.list)) {#
  # Start feedback:#
  cat("Attempting to read: ", file.list[i], "...")#
  # Get stripped verion of name (i.e., missing a, b, aa etc. ending):#
  stripped_name <- gsub(strsplit(nexus.files[i], "[:0-9:]{4}|inpress")[[1]][2], "", nexus.files[i])#
  # Get hits for stripped name in filenames:#
  hits <- grep(stripped_name, filenames)#
  # Check there is a match:#
  if(length(hits) == 0) stop("No reference with matching name.")#
  # Create reference info:#
  reference_info <- paste(results[hits], collapse = "\n\nOR\n\n")#
  # If multiple hits add to list so these can be manually checked later:#
  if(length(hits) > 1) multi_hitters <- c(multi_hitters, nexus.files[i])#
  # Read in matrix:#
  mymatrix <- ReadMorphNexus(file.list[i])#
  # Update header text:#
  mymatrix$Topper$Header <- "File downloaded from graemetlloyd.com"#
  # Make file name:#
  file.name <- gsub(".nex", "", strsplit(file.list[i], "/")[[1]][length(strsplit(file.list[i], "/")[[1]])])#
  # Write out NEXUS data:#
  WriteMorphNexus(CladisticMatrix = mymatrix, filename = paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/nexus", "/", file.name, ".nex", sep = ""))#
  # Write out TNT data:#
  WriteMorphTNT(CladisticMatrix = mymatrix, filename = paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/tnt", "/", file.name, ".tnt", sep = ""))#
  # Write out TNT for analysis:#
  WriteMorphTNT(CladisticMatrix = mymatrix, filename = paste("/Users/eargtl", "/", file.name, ".tnt", sep = ""), add.analysis.block = TRUE)#
  TNTFA <- readLines(paste("/Users/eargtl", "/", file.name, ".tnt", sep = ""))#
  # If scratch.tre is found:#
  if(length(grep("scratch.tre", TNTFA, fixed = TRUE)) > 0) {#
    # Replace scratch.tre with numbered version:#
    TNTFA <- gsub("scratch.tre", paste("scratch", scratch_counter, ".tre", sep = ""), TNTFA, fixed = TRUE)#
    # Overwrite TNT for analysis with numbered scratch.tre:#
    write(TNTFA, paste("/Users/eargtl", "/", file.name, ".tnt", sep = ""))#
    # Increment scratch counter:#
    scratch_counter <- scratch_counter + 1#
  }#
  # Make XML file:#
  myxml <- paste(paste("<?xml version=\"1.0\" standalone=\"yes\"?>\n<SourceTree>\n\t<Source>\n", reference_info, "\t</Source>"), paste("\t<Taxa number=\"", length(mymatrix$Matrix_1$Matrix[, 1]), "\">", sep = ""), paste(paste("\t\t<List recon_name=\"DELETE\" recon_no=\"-1\">", rownames(mymatrix$Matrix_1$Matrix), "</List>", sep = ""), collapse = "\n"), "\t</Taxa>\n\t<Characters>\n\t\t<Molecular/>", paste("\t\t<Morphological number=\"", sum(unlist(lapply(lapply(mymatrix[2:length(mymatrix)], '[[', "Matrix"), ncol))), "\">", sep = ""), "\t\t\t<Type>Osteology</Type>\n\t\t</Morphological>\n\t\t<Behavioural/>\n\t\t<Other/>\n\t</Characters>\n\t<Analysis>\n\t\t<Type>Maximum Parsimony</Type>\n\t</Analysis>\n\t<Notes>Based on reanalysis of the original matrix.</Notes>", paste("\t<Filename>", gsub("\\.nex", "", strsplit(file.list[i], "/")[[1]][length(strsplit(file.list[i], "/")[[1]])]), "</Filename>", sep = ""), "\t<Parent/>\n\t<Sibling/>\n</SourceTree>", sep = "\n")#
  # Write out XML file:#
  write(myxml, paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/xml", "/", file.name, ".xml", sep = ""))#
  # Feedback:#
  cat("Done\n")#
}#
#
# List multiple hitters for checking:#
sort(multi_hitters)
# Open libraries:#
library(Claddis)#
library(metatree)#
#
# Set working directory:#
setwd("/Users/eargtl")#
#
# Get list of mrp files:#
mrp.list <- list.files()[grep("mrp.nex", list.files())]#
#
# Get list of mrp files:#
trees.list <- list.files()[grep("mpts_plus_strict.nex", list.files())]#
#
# Make tree files:#
for(i in 1:length(trees.list)) {#
  # Read in TNT trees and split into mpts and strict consensus:#
  mytrees <- Trees2MPTsAndStrict(trees.list[i])#
  # If the tree limit of 100000 was hit (i.e., not all MPTs are guranteed to have been sampled) or no MRP could be created due to sheer number of trees:#
  if(length(mytrees$mpts) == 100000 || sum(mrp.list == gsub("tntmpts_plus_strict.nex", "mrp.nex", trees.list[i])) == 0) {#
    # Create MRP filename:#
    mrp.filename <- gsub("tntmpts_plus_strict.nex", "mrp.nex", trees.list[i])#
    # If MRP file was generated:#
    if(length(which(mrp.list == mrp.filename)) > 0) {#
      # Remove from MRP list:#
      mrp.list <- mrp.list[-which(mrp.list == mrp.filename)]#
      # Delete raw file as likely too big anyway:#
      file.remove(mrp.filename)#
    }#
    # Create nexus file name:#
    nexus.filename <- gsub("mrp\\.nex", ".nex", mrp.filename)#
    # Read in original matrix:#
    mymatrix <- ReadMorphNexus(paste("~/Documents/Homepage/www.graemetlloyd.com/nexus", "/", nexus.filename, sep = ""))#
    # Write out regular TNT file:#
    WriteMorphTNT(mymatrix, gsub("\\.nex", ".tnt", nexus.filename))#
    # Read in TNT lines:#
    TNT.lines <- readLines(gsub("\\.nex", ".tnt", nexus.filename))#
    # Get TNT data block:#
    tnt.block <- TNT.lines[1:(grep("proc/;", TNT.lines) - 1)]#
    # Create analysis block:#
    anal.block <- paste(c("rseed*;\nhold 999;\nxmult=rss fuse 50 drift 50 ratchet 50;\nmult 50 =tbr drift;\ntsave scratch.tre;\nsave;\ntsave /;", rep("rseed*;\nhold 999;\nxmult=rss fuse 50 drift 50 ratchet 50;\nmult 50 =tbr drift;\ntsave scratch.tre +;\nsave;\ntsave /;", 4), "hold 5000;\nshortread scratch.tre;\nbbreak=tbr;"), collapse = "\n")#
    # Cretae empty vector to store final block:#
    full.block <- vector(mode = "character")#
    # Fill out all blocks for analysis:#
    for(j in 1:20) full.block <- c(full.block, paste(paste(tnt.block, collapse = "\n"), anal.block, "mrp;", paste("export ", gsub("\\.nex", "", nexus.filename), "mrp_", j, ".nex;", sep = ""), sep = "\n"))#
    # Write out TNT file:#
    write(paste(paste(full.block, collapse = "\n"), "\nproc/;\n", sep = ""), gsub("\\.nex", ".tnt", nexus.filename))#
  }#
  # Make file name:#
  file.name <- gsub("tntmpts_plus_strict.nex", "", trees.list[i])#
  # Write out MPTs:#
  write(mytrees$mpts, paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/mpts", "/", file.name, ".tre", sep = ""))#
  # Write out first MPT:#
  write(mytrees$mpts[1], paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/firstmpt", "/", file.name, ".tre", sep = ""))#
  # Write out strict consensus:#
  write(mytrees$strict, paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/sc", "/", file.name, ".tre", sep = ""))#
  # Delete trees file now no longer needed:#
  file.remove(trees.list[i])#
  # Spit out loop position:#
  cat(i, " ")#
}#
#
# Make mrp files:#
for(i in 1:length(mrp.list)) {#
  # Add assumptions block to MRP:#
  x <- paste(c(readLines(mrp.list[i]), "BEGIN ASSUMPTIONS;", "OPTIONS  DEFTYPE=unord PolyTcount=MINSTEPS ;", "END;"), collapse = "\n")#
  # Write out MRP file with assumptions added (can then be read in with ReadMorphNexus):#
  write(x = x, file = mrp.list[i])#
  # Read in MRP file:#
  mymrp <- ReadMorphNexus(mrp.list[i])#
  # Remove root taxon:#
  mymrp$Matrix_1$Matrix <- mymrp$Matrix_1$Matrix[-which(rownames(mymrp$Matrix_1$Matrix) == "ROOT"), ]#
  # Collapse to just unique characters:#
  mymrp <- CompactifyMatrix(mymrp)#
  # Overwrite weights (set all to one):#
  mymrp$Matrix_1$Weights <- rep(1, length(mymrp$Matrix_1$Weights))#
  # Make file name:#
  file.name <- gsub(".nex", "", mrp.list[i])#
  # Isolate MPR taxon names:#
  mrp.names <- rownames(mymrp$Matrix_1$Matrix)#
  # Isolate full names:#
  nexus.names <- rownames(ReadMorphNexus(paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/nexus/", gsub("mrp", "", file.name), ".nex", sep = ""))$Matrix_1$Matrix)#
  # Check to see if MRP names are contracted:#
  if(length(setdiff(mrp.names, nexus.names)) > 0) {#
    # List all contracted names:#
    contracted.names <- setdiff(mrp.names, nexus.names)#
    # For each contracted name:#
    for(j in 1:length(contracted.names)) {#
      # Get matching full name(s):#
      full.name <- nexus.names[grep(contracted.names[j], nexus.names)]#
      # Check that there are not multiple matches:#
      if(length(full.name) > 1) stop("Multiple names match contracted form. Check manually.")#
      # Overwrite contracted name with full name:#
      rownames(mymrp$Matrix_1$Matrix)[which(rownames(mymrp$Matrix_1$Matrix) == contracted.names[j])] <- full.name#
    }#
  }#
  # Write out MRP in #NEXUS format:#
  WriteMorphNexus(mymrp, paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/mrp", "/", file.name, ".nex", sep = ""))#
  # Delete file once finished:#
  file.remove(mrp.list[i])#
  # Spit out loop position:#
  cat(i, " ")#
}
i
mrp.list[i]
# Make mrp files:#
for(i in 5:length(mrp.list)) {#
  # Add assumptions block to MRP:#
  x <- paste(c(readLines(mrp.list[i]), "BEGIN ASSUMPTIONS;", "OPTIONS  DEFTYPE=unord PolyTcount=MINSTEPS ;", "END;"), collapse = "\n")#
  # Write out MRP file with assumptions added (can then be read in with ReadMorphNexus):#
  write(x = x, file = mrp.list[i])#
  # Read in MRP file:#
  mymrp <- ReadMorphNexus(mrp.list[i])#
  # Remove root taxon:#
  mymrp$Matrix_1$Matrix <- mymrp$Matrix_1$Matrix[-which(rownames(mymrp$Matrix_1$Matrix) == "ROOT"), ]#
  # Collapse to just unique characters:#
  mymrp <- CompactifyMatrix(mymrp)#
  # Overwrite weights (set all to one):#
  mymrp$Matrix_1$Weights <- rep(1, length(mymrp$Matrix_1$Weights))#
  # Make file name:#
  file.name <- gsub(".nex", "", mrp.list[i])#
  # Isolate MPR taxon names:#
  mrp.names <- rownames(mymrp$Matrix_1$Matrix)#
  # Isolate full names:#
  nexus.names <- rownames(ReadMorphNexus(paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/nexus/", gsub("mrp", "", file.name), ".nex", sep = ""))$Matrix_1$Matrix)#
  # Check to see if MRP names are contracted:#
  if(length(setdiff(mrp.names, nexus.names)) > 0) {#
    # List all contracted names:#
    contracted.names <- setdiff(mrp.names, nexus.names)#
    # For each contracted name:#
    for(j in 1:length(contracted.names)) {#
      # Get matching full name(s):#
      full.name <- nexus.names[grep(contracted.names[j], nexus.names)]#
      # Check that there are not multiple matches:#
      if(length(full.name) > 1) stop("Multiple names match contracted form. Check manually.")#
      # Overwrite contracted name with full name:#
      rownames(mymrp$Matrix_1$Matrix)[which(rownames(mymrp$Matrix_1$Matrix) == contracted.names[j])] <- full.name#
    }#
  }#
  # Write out MRP in #NEXUS format:#
  WriteMorphNexus(mymrp, paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/mrp", "/", file.name, ".nex", sep = ""))#
  # Delete file once finished:#
  file.remove(mrp.list[i])#
  # Spit out loop position:#
  cat(i, " ")#
}
i=4
# Add assumptions block to MRP:#
  x <- paste(c(readLines(mrp.list[i]), "BEGIN ASSUMPTIONS;", "OPTIONS  DEFTYPE=unord PolyTcount=MINSTEPS ;", "END;"), collapse = "\n")
x
# Read in MRP file:#
  mymrp <- ReadMorphNexus(mrp.list[i])
# Remove root taxon:#
  mymrp$Matrix_1$Matrix <- mymrp$Matrix_1$Matrix[-which(rownames(mymrp$Matrix_1$Matrix) == "ROOT"), ]
# Collapse to just unique characters:#
  mymrp <- CompactifyMatrix(mymrp)
# Overwrite weights (set all to one):#
  mymrp$Matrix_1$Weights <- rep(1, length(mymrp$Matrix_1$Weights))
# Make file name:#
  file.name <- gsub(".nex", "", mrp.list[i])
# Isolate MPR taxon names:#
  mrp.names <- rownames(mymrp$Matrix_1$Matrix)
# Isolate full names:#
  nexus.names <- rownames(ReadMorphNexus(paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/nexus/", gsub("mrp", "", file.name), ".nex", sep = ""))$Matrix_1$Matrix)
ReadMorphNexus(paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/nexus/", gsub("mrp", "", file.name), ".nex", sep = ""))
gsub("mrp", "", file.name)
ReadMorphNexus(paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/nexus/", gsub("mrp", "", file.name), ".nex", sep = ""))
# Read in MRP file:#
  mymrp <- ReadMorphNexus(mrp.list[i])#
  # Remove root taxon:#
  mymrp$Matrix_1$Matrix <- mymrp$Matrix_1$Matrix[-which(rownames(mymrp$Matrix_1$Matrix) == "ROOT"), ]#
  # Collapse to just unique characters:#
  mymrp <- CompactifyMatrix(mymrp)#
  # Overwrite weights (set all to one):#
  mymrp$Matrix_1$Weights <- rep(1, length(mymrp$Matrix_1$Weights))
# Make file name:#
  file.name <- gsub(".nex", "", mrp.list[i])#
  # Isolate MPR taxon names:#
  mrp.names <- rownames(mymrp$Matrix_1$Matrix)
# Write out MRP in #NEXUS format:#
  WriteMorphNexus(mymrp, paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/mrp", "/", file.name, ".nex", sep = ""))#
  # Delete file once finished:#
  file.remove(mrp.list[i])
library(Claddis)
x <- ReadMorphNexus("~/Documents/Homepage/www.graemetlloyd.com/nexus/Hartman_etal_2019a.nex")
y <- SafeTaxonomicReduction(x)
names(y)
y$removed.matrix
# Get functions in:#
library(Claddis)#
library(ade4)#
library(foreach)#
library(doParallel)#
#
# Register parallel back end as number of cores available:#
registerDoParallel(cores = 4)#
#
# Set working directory:#
setwd("/Users/eargtl")#
#
# Get list of mrp files:#
mrp.list <- list.files()[grep("mrp_", list.files())]#
#
# For each MRP file:#
for(i in 1:length(mrp.list)) {#
  # Read in raw MRP file:#
  x <- readLines(mrp.list[i])#
  # If there is no assumptions block:#
  if(length(grep("begin assumptions", x, ignore.case = TRUE)) == 0) {#
    # Add assumptions block to MRP:#
    x <- paste(c(x, "BEGIN ASSUMPTIONS;", "OPTIONS  DEFTYPE=unord PolyTcount=MINSTEPS ;", "END;"), collapse = "\n")#
    # Write out MRP file with assumptions added (can then be read in with ReadMorphNexus):#
    write(x = x, file = mrp.list[i])#
  }#
}#
#
# Make mrp files:#
x <- foreach(i = 1:length(mrp.list), .combine = "rbind") %dopar% {#
#
  # Read in ith MRP file:#
  mymrp <- ReadMorphNexus(mrp.list[i])#
  # Compactify the matrix:#
  mymrp <- CompactifyMatrix(mymrp)#
  # To avoid non-numeric weight (e.g., 1+e05) set all weights above ten to ten:#
  if(sum(mymrp$Matrix_1$Weights > 10)) mymrp$Matrix_1$Weights[which(mymrp$Matrix_1$Weights > 10)] <- 10#
  # If any rogue NAs are found prune these from the data:#
  if(length(unique(as.vector(mymrp$Matrix_1$Matrix))) > 2) mymrp <- MatrixPruner(mymrp, characters2prune = which((apply(apply(mymrp$Matrix_1$Matrix, 2, '==', "0") + apply(mymrp$Matrix_1$Matrix, 2, '==', "1"), 2, sum)) < nrow(mymrp$Matrix_1$Matrix)))#
#
  # Overwrite original data with compactified version:#
  WriteMorphNexus(mymrp, mrp.list[i])#
}#
#
# Get unique data set names:#
data.sets <- unique(matrix(unlist(strsplit(mrp.list, "mrp_")), ncol = 2, byrow = TRUE)[, 1])#
#
# For each data set:#
for(i in 1:length(data.sets)) {#
  # Get numbers for files to read in:#
  files.to.load <- grep(data.sets[i], mrp.list)#
  # For each file in data set:#
  for(j in files.to.load) {#
    # Read in current matrix:#
    current.matrix <- ReadMorphNexus(mrp.list[j])#
    # Sort by row name to ensure taxa line up later:#
    current.matrix$Matrix_1$Matrix <- current.matrix$Matrix_1$Matrix[sort(rownames(current.matrix$Matrix_1$Matrix)), ]#
    # If first file of data set:#
    if(files.to.load[1] == j) {#
      # Set matrix using current matrix:#
      MATRIX <- current.matrix$Matrix_1$Matrix#
      # Set weights using current matrix:#
      WEIGHTS <- current.matrix$Matrix_1$Weights#
    # If not first file of data set:#
    } else {#
      # Add current matrix to data set:#
      MATRIX <- cbind(MATRIX, current.matrix$Matrix_1$Matrix)#
      # Add current weights to data set:#
      WEIGHTS <- c(WEIGHTS, current.matrix$Matrix_1$Weights)#
    }#
  }#
  # Overwrite current matrix with full data set:#
  current.matrix$Matrix_1$Matrix <- MATRIX#
  # Overwrite current matrix weights with full data set:#
  current.matrix$Matrix_1$Weights <- WEIGHTS#
  # Set ordering for full data set:#
  current.matrix$Matrix_1$Ordering <- rep("unord", ncol(current.matrix$Matrix_1$Matrix))#
  # Set maximum values for full data set:#
  current.matrix$Matrix_1$MinVals <- rep(1, ncol(current.matrix$Matrix_1$Matrix))#
  # Set minimum values for full data set:#
  current.matrix$Matrix_1$MaxVals <- rep(0, ncol(current.matrix$Matrix_1$Matrix))#
  # Collapse data set:#
  current.matrix <- CompactifyMatrix(current.matrix)#
  # To avoid non-numeric weight (e.g., 1+e05) set all weights above ten to ten:#
  if(sum(current.matrix$Matrix_1$Weights > 10)) current.matrix$Matrix_1$Weights[which(current.matrix$Matrix_1$Weights > 10)] <- 10#
  # Make file name:#
  file.name <- data.sets[i]#
  # Case if MRP is done (minimum weight is greater than 1):#
  if(min(current.matrix$Matrix_1$Weights) > 1) {#
    # Remove "ROOT" taxon if present:#
    if(sum(rownames(current.matrix$Matrix_1$Matrix) == "ROOT") > 0) current.matrix$Matrix_1$Matrix <- current.matrix$Matrix_1$Matrix[-which(rownames(current.matrix$Matrix_1$Matrix) == "ROOT"), ]#
    # Collapse matrix again:#
    current.matrix <- CompactifyMatrix(current.matrix)#
    # Overwrite all weights with 1:#
    current.matrix$Matrix_1$Weights <- rep(1, length(current.matrix$Matrix_1$Weights))#
    # Update matrix in nesting order (outgroup first):#
    current.matrix$Matrix_1$Matrix <- current.matrix$Matrix_1$Matrix[names(sort(apply(apply(current.matrix$Matrix_1$Matrix, 1, as.numeric), 2, sum))), ]#
    # Isolate MRP taxon names:#
    mrp.names <- rownames(current.matrix$Matrix_1$Matrix)#
    # Isolate full names:#
    nexus.names <- rownames(ReadMorphNexus(paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/nexus/", gsub("mrp", "", file.name), ".nex", sep = ""))$Matrix_1$Matrix)#
    # Check to see if MRP names are contracted:#
    if(length(setdiff(mrp.names, nexus.names)) > 0) {#
      # List all contracted names:#
      contracted.names <- setdiff(mrp.names, nexus.names)#
      # For each contracted name:#
      for(j in 1:length(contracted.names)) {#
        # Get matching full name(s):#
        full.name <- nexus.names[grep(contracted.names[j], nexus.names)]#
        # Check that there are not multiple matches:#
        if(length(full.name) > 1) stop("Multiple names match contracted form. Check manually.")#
        # Overwrite contracted name with full name:#
        rownames(current.matrix$Matrix_1$Matrix)[which(rownames(current.matrix$Matrix_1$Matrix) == contracted.names[j])] <- full.name#
      }#
    }#
    # Write out MRP in #NEXUS format:#
    WriteMorphNexus(current.matrix, paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/mrp", "/", file.name, "mrp.nex", sep = ""))#
    # Remove dead files:#
    file.remove(c(mrp.list[files.to.load], paste(file.name, ".tnt", sep = "")))#
  # Case if MRP needs to continue (minimum weight is 1):#
  } else {#
    # Remove dead files:#
    file.remove(mrp.list[files.to.load])#
    # Write out MRP in #NEXUS format:#
    WriteMorphNexus(current.matrix, paste("/Users/eargtl/", file.name, "mrp_0.nex", sep = ""))#
  }#
  # Output loop position:#
  cat(i, " ")#
}
# CODE TO FILL OUT SAFE FIRST GUESSES FOR TAXON RECONCILIATION WHERE MISSING FROM XML FILES#
# I.E., USES EXACT SPECIES NAMES AND PALEOBIOLOGY DATABASE OR USES RECONCILIATION FROM PARENT DATASET FOR EXACT SAME OTU OTHERWISE OTUS ARE NOT ALTERED AT ALL#
#
# Load libraries:#
library(metatree)#
library(Claddis)#
#
# Set working directory to HTML files:#
setwd("~/Documents/Homepage/www.graemetlloyd.com")#
#
# Get list of matrix HTMLs:#
MatrixHTMLfiles <- list.files()[grep("matr[:a-z:]{4}.html", list.files())]#
#
# Create empty HTML list:#
MatrixHTML <- list()#
#
# Read in each raw HTML code into list:#
for(i in 1:length(MatrixHTMLfiles)) MatrixHTML[[i]] <- readLines(MatrixHTMLfiles[i])#
#
# Add file names to HTML list:#
names(MatrixHTML) <- gsub(",html", "", MatrixHTMLfiles)#
#
# Set working directory as XML folder:#
setwd("~/Documents/Homepage/www.graemetlloyd.com/xml")#
#
# List all XML files:#
xmlfiles <- list.files()#
#
# Create empty vector to store parent datasets:#
parentdataset <- vector(mode = "character")#
#
# For each XML file:#
for(i in xmlfiles) {#
    # Read in ith XML file:#
    currentxml <- readLines(i)#
    # If there is a parent data set (from which to draw recinciliation data from):#
    parentdataset <- c(parentdataset, ifelse(length(grep("<Parent>", currentxml)) > 0, strsplit(currentxml[grep("<Parent>", currentxml)], "<|>")[[1]][3], ""))#
}#
#
# Add file names to parent data set vector:#
names(parentdataset) <- gsub(".xml", "", xmlfiles)#
#
# Get vector of "dead" parents, i.e., those not currently present in the data pool:#
deadparents <- sort(setdiff(unique(parentdataset), names(parentdataset)))[which(nchar(sort(setdiff(unique(parentdataset), names(parentdataset)))) > 0)]#
#
# Remove dead parents from parent list:#
for(i in deadparents) parentdataset[which(parentdataset == i)] <- ""#
#
# Create empty parent depth vector (number of links to original data set):#
parentdepth <- vector(mode = "numeric")#
#
# For each data set:#
for(i in names(parentdataset)) {#
    # Set starting depth at zero (no parent at all):#
    currentdepth <- 0#
    # If there is at least an initial parent:#
    if(parentdataset[i] != "") {#
        # Increase depth by one:#
        currentdepth <- currentdepth + 1#
        # While there are further parents:#
        while(parentdataset[i] != "") {#
            # Increase depth by one#
            currentdepth <- currentdepth + 1#
            # Update new parent:#
            i <- parentdataset[i]#
        }#
    }#
    # Add parent depth to vector:#
    parentdepth <- c(parentdepth, currentdepth)#
}#
#
# Reorder xml file lists by parent depth (ensures parents are filled out before children so names can always be carried forwards):#
xmlfiles <- xmlfiles[order(parentdepth, decreasing = FALSE)]#
#
# Count of total OTUs (start with zero):#
TotalOTUs <- 0#
#
# For each XML file:#
for(i in xmlfiles) {#
    # Read in ith XML file:#
    currentxml <- readLines(i)#
    # If there is a parent data set (from which to draw recinciliation data from):#
    if(length(grep("<Parent>", currentxml)) > 0) {#
        # Get parent data set file name:#
        parentdataset <- strsplit(currentxml[grep("<Parent>", currentxml)], "<|>")[[1]][3]#
        # Check parent has been processed (has an XML file on which to draw):#
        if(!is.na(match(paste(parentdataset, ".xml", sep = ""), xmlfiles))) {#
            # Isolate taxon names block:#
            taxonnameblock <- currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)]#
            # Reformat as matrix:#
            taxonnameblock <- matrix(unlist(lapply(strsplit(taxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE, dimnames = list(c(), c("ReconName", "ReconNo", "OTUName")))#
            # Add number of OTUs to count:#
            TotalOTUs <- TotalOTUs + nrow(taxonnameblock)#
            # If there are unreconciled taxa:#
            if(any(taxonnameblock[, "ReconNo"] == "-1")) {#
                # Get just the unreconciled names (we don't care about ones already done):#
                unreconcilednames <- taxonnameblock[which(taxonnameblock[, "ReconNo"] == "-1"), "OTUName"]#
                # Read in ith XML file:#
                parentxml <- readLines(paste(parentdataset, ".xml", sep = ""))#
                # Isolate taxon names block:#
                parenttaxonnameblock <- parentxml[(grep("<Taxa", parentxml) + 1):(grep("</Taxa", parentxml) - 1)]#
                # Reformat as matrix:#
                parenttaxonnameblock <- matrix(unlist(lapply(strsplit(parenttaxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE, dimnames = list(c(), c("ReconName", "ReconNo", "OTUName")))#
                # If at least one name can be reconciled using parent data set data:#
                if(any(!is.na(match(unreconcilednames, parenttaxonnameblock[, "OTUName"])))) {#
                    # Update unreconciled names as just those also present in parent data:#
                    unreconcilednames <- unreconcilednames[!is.na(match(unreconcilednames, parenttaxonnameblock[, "OTUName"]))]#
#
                    # Update taxon names block with parent data:#
                    taxonnameblock[match(unreconcilednames, taxonnameblock[, "OTUName"]), ] <- parenttaxonnameblock[match(unreconcilednames, parenttaxonnameblock[, "OTUName"]), ]#
                }#
            }#
            # Add taxonblock back into currentxml:#
            currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)] <- paste("\t\t<List recon_name=\"", taxonnameblock[, "ReconName"], "\" recon_no=\"", taxonnameblock[, "ReconNo"], "\">", taxonnameblock[, "OTUName"], "</List>", sep = "")#
#
        }#
#
    }#
    # Write out XML:#
    write(x = paste(currentxml, collapse = "\n"), file = i)#
}#
#
# Set empty priorites matrix:#
Priorities <- matrix(nrow = 0, ncol = 4, dimnames = list(c(), c("Name", "NUnreconciled", "ParentDepth", "HTMLFile")))#
#
# For each XML file:#
for(i in xmlfiles) {#
  # Read in ith XML file:#
  currentxml <- readLines(i)#
  # Work out which html file the data set belongs to:#
  currenthtml <- names(which(unlist(lapply(lapply(MatrixHTML, grep, pattern = paste("xml/", i, sep = "")), length)) == 1))#
  # Isolate taxon names block:#
  taxonnameblock <- currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)]#
  # Reformat as matrix:#
  taxonnameblock <- matrix(unlist(lapply(strsplit(taxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE, dimnames = list(c(), c("ReconName", "ReconNo", "OTUName")))#
  # Add to priorities matrix:#
  Priorities <- rbind(Priorities, c(i, sum(taxonnameblock[, "ReconNo"] == "-1"), parentdepth[order(parentdepth, decreasing = FALSE)][which(xmlfiles == i)], currenthtml))#
}#
#
# Remove any XMLs where all taxa are already reconciled:#
Priorities <- Priorities[-which(Priorities[, "NUnreconciled"] == "0"), , drop = FALSE]#
#
# Number of files that still need taxa reconciled:#
paste(nrow(Priorities), " files still contain unreconciled taxa (", round(((length(xmlfiles) - nrow(Priorities)) / length(xmlfiles) * 100), 2), "% complete)", sep = "")#
#
# Number of individual OTU names that still need reconciling:#
paste(sum(as.numeric(Priorities[, "NUnreconciled"])), " OTUs are still unreconciled (", round(((TotalOTUs - sum(as.numeric(Priorities[, "NUnreconciled"]))) / TotalOTUs * 100), 2), "% complete)", sep = "")#
#
# Order by number of unreconciled OTUs:#
Priorities <- Priorities[order(as.numeric(Priorities[, "NUnreconciled"])), ]#
#
# Display priorities for archosaurs (excluding Cenozoic birds):#
Priorities[sort(c(which(Priorities[, "HTMLFile"] == "matrarch.html"), which(Priorities[, "HTMLFile"] == "matrdino.html"))), ]#
#
# Empty vector to store taxon names:#
newtaxonnames <- vector(mode = "character")#
#
# For each XML file:#
for(i in xmlfiles) {#
#
    # Read in ith XML file:#
    currentxml <- readLines(i)#
    # Isolate taxon names block:#
    taxonnameblock <- currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)]#
    # Convert into matrix:#
    taxonnameblock <- matrix(unlist(lapply(strsplit(taxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE)#
    # Add any unique taxon names to the vector:#
    newtaxonnames <- sort(unique(c(newtaxonnames, gsub("_", " ", taxonnameblock[which(taxonnameblock[, 2] == "-1"), 3]))))#
#
}#
#
# Create empty resolved names matrix:#
resolvednames <- matrix(nrow = 0, ncol = 9, dimnames = list(c(), c("InputName", "OriginalTaxonNo", "ResolvedTaxonNo", "TaxonName", "TaxonRank", "ParentTaxonNo", "TaxonValidity", "AcceptedNumber", "AcceptedName")))#
#
# Get unique long names (i.e., those with a space in them):#
UniqueLongNames <- newtaxonnames[grep(" ", newtaxonnames)]#
#
# Get down to just binomials:#
UniqueLongNames <- UniqueLongNames[unlist(lapply(strsplit(UniqueLongNames, split = " "), length)) == 2]#
#
# Get rid of any "sp" names:#
UniqueLongNames <- UniqueLongNames[!unlist(lapply(lapply(strsplit(UniqueLongNames, split = " "), '==', "sp"), any))]#
#
# Get rid of any "sp" names:#
UniqueLongNames <- UniqueLongNames[!unlist(lapply(lapply(strsplit(UniqueLongNames, split = " "), '==', "indet"), any))]#
#
# Remove names with numbers in them (if found):#
if(length(grep("[:0-9:]", UniqueLongNames)) > 0) UniqueLongNames <- UniqueLongNames[-grep("[:0-9:]", UniqueLongNames)]#
#
# Cut out any names with only a single character part (can't be a proper name):#
UniqueLongNames <- UniqueLongNames[!unlist(lapply(lapply(lapply(strsplit(UniqueLongNames, split = " "), nchar), '==', 1), any))]#
#
# For each (potential) species name (i.e., names containing a space):#
for(i in UniqueLongNames) {#
    # Set empty variable x as NA:#
    x <- NA#
    # Attempt to query current name in database:#
    try(x <- PaleobiologyDBTaxaQuerier(taxon_no = "1", taxon_name = i), silent = TRUE)#
    # If query works then add to resolved names matrix:#
    if(length(x) > 1) resolvednames <- rbind(resolvednames, c(i, unlist(x)))#
    # Close all connections (stops limiting factor in R):#
    closeAllConnections()#
    # Spit out loop position (i.e., current name attempting to reconcile):#
    cat(i, "\n")#
#
}#
#
# Check names match up (or remove those that return a different name):#
resolvednames <- resolvednames[which(resolvednames[, "TaxonName"] == resolvednames[, "InputName"]), ]#
#
# Check names are only regular genus species (no subgenera or subspecies) as I am not prepared to deal with these yet!:#
resolvednames <- resolvednames[which(nchar(gsub("[:A-Z:a-z:]", "", resolvednames[, "TaxonName"])) == 1), ]#
#
# For each XML file:#
for(i in xmlfiles) {#
    # Read in ith XML file:#
    currentxml <- readLines(i)#
    # Isolate taxon names block:#
    taxonnameblock <- currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)]#
    # For each taxon in block:#
    for(j in 1:length(taxonnameblock)) {#
        # Isolate just elements of taxonomic reconciliation from taxon string:#
        splitdata <- strsplit(gsub("\"", "", taxonnameblock[j], fixed = TRUE), "recon_name=| |recon_no=|<|>")[[1]][c(4, 6, 7)]#
        # Split out data into subvariables:#
        currentreconname <- splitdata[1]#
        currentreconnumber <- splitdata[2]#
        originalOTUname <- splitdata[3]#
        # If taxon not currently reconciled:#
        if(currentreconnumber == "-1") {#
            # Check to see if in resolved names list (returns row number if found):#
            rownumber <- which(resolvednames[, 1] == gsub("_", " ", originalOTUname))#
            # If a single reconciled name found:#
            if(length(rownumber) == 1) {#
                # Update reconnumber with new information:#
                currentreconnumber <- gsub("txn:|var:", "", rev(sort(resolvednames[rownumber, c("OriginalTaxonNo", "ResolvedTaxonNo")]))[[1]])#
                # Update taxon name block:#
                taxonnameblock[j] <- paste("\t\t<List recon_name=\"", originalOTUname, "\" recon_no=\"", currentreconnumber, "\">", originalOTUname, "</List>", sep = "")#
            }#
        }#
    }#
    # Update current xml with new taxon name block:#
    currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)] <- taxonnameblock#
#
    # Write out new XML:#
    write(x = paste(currentxml, collapse = "\n"), file = i)#
#
}#
#
# Create empty to do list of remaining data sets:#
newtodolist <- vector(mode = "numeric")#
#
# For each XML file:#
for(i in xmlfiles) {#
    # Read in ith XML file:#
    currentxml <- readLines(i)#
    # Isolate taxon names block:#
    taxonnameblock <- currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)]#
    # Convert into matrix:#
    taxonnameblock <- matrix(unlist(lapply(strsplit(taxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE)#
    # If data set has any unresolved names:#
    if(any(taxonnameblock[, 2] == "-1")) {#
        # Count number still to do and add to list:#
        newtodolist <- c(newtodolist, sum(taxonnameblock[, 2] == "-1"))#
        # Add data set name to list:#
        names(newtodolist)[length(newtodolist)] <- i#
        # Reorder list:#
        newtodolist <- newtodolist[order(newtodolist)]#
    }#
}
# CODE TO FILL OUT SAFE FIRST GUESSES FOR TAXON RECONCILIATION WHERE MISSING FROM XML FILES#
# I.E., USES EXACT SPECIES NAMES AND PALEOBIOLOGY DATABASE OR USES RECONCILIATION FROM PARENT DATASET FOR EXACT SAME OTU OTHERWISE OTUS ARE NOT ALTERED AT ALL#
#
# Load libraries:#
library(metatree)#
library(Claddis)#
#
# Set working directory to HTML files:#
setwd("~/Documents/Homepage/www.graemetlloyd.com")#
#
# Get list of matrix HTMLs:#
MatrixHTMLfiles <- list.files()[grep("matr[:a-z:]{4}.html", list.files())]#
#
# Create empty HTML list:#
MatrixHTML <- list()#
#
# Read in each raw HTML code into list:#
for(i in 1:length(MatrixHTMLfiles)) MatrixHTML[[i]] <- readLines(MatrixHTMLfiles[i])#
#
# Add file names to HTML list:#
names(MatrixHTML) <- gsub(",html", "", MatrixHTMLfiles)#
#
# Set working directory as XML folder:#
setwd("~/Documents/Homepage/www.graemetlloyd.com/xml")#
#
# List all XML files:#
xmlfiles <- list.files()#
#
# Create empty vector to store parent datasets:#
parentdataset <- vector(mode = "character")#
#
# For each XML file:#
for(i in xmlfiles) {#
    # Read in ith XML file:#
    currentxml <- readLines(i)#
    # If there is a parent data set (from which to draw recinciliation data from):#
    parentdataset <- c(parentdataset, ifelse(length(grep("<Parent>", currentxml)) > 0, strsplit(currentxml[grep("<Parent>", currentxml)], "<|>")[[1]][3], ""))#
}#
#
# Add file names to parent data set vector:#
names(parentdataset) <- gsub(".xml", "", xmlfiles)#
#
# Get vector of "dead" parents, i.e., those not currently present in the data pool:#
deadparents <- sort(setdiff(unique(parentdataset), names(parentdataset)))[which(nchar(sort(setdiff(unique(parentdataset), names(parentdataset)))) > 0)]#
#
# Remove dead parents from parent list:#
for(i in deadparents) parentdataset[which(parentdataset == i)] <- ""#
#
# Create empty parent depth vector (number of links to original data set):#
parentdepth <- vector(mode = "numeric")#
#
# For each data set:#
for(i in names(parentdataset)) {#
    # Set starting depth at zero (no parent at all):#
    currentdepth <- 0#
    # If there is at least an initial parent:#
    if(parentdataset[i] != "") {#
        # Increase depth by one:#
        currentdepth <- currentdepth + 1#
        # While there are further parents:#
        while(parentdataset[i] != "") {#
            # Increase depth by one#
            currentdepth <- currentdepth + 1#
            # Update new parent:#
            i <- parentdataset[i]#
        }#
    }#
    # Add parent depth to vector:#
    parentdepth <- c(parentdepth, currentdepth)#
}#
#
# Reorder xml file lists by parent depth (ensures parents are filled out before children so names can always be carried forwards):#
xmlfiles <- xmlfiles[order(parentdepth, decreasing = FALSE)]#
#
# Count of total OTUs (start with zero):#
TotalOTUs <- 0#
#
# For each XML file:#
for(i in xmlfiles) {#
    # Read in ith XML file:#
    currentxml <- readLines(i)#
    # If there is a parent data set (from which to draw recinciliation data from):#
    if(length(grep("<Parent>", currentxml)) > 0) {#
        # Get parent data set file name:#
        parentdataset <- strsplit(currentxml[grep("<Parent>", currentxml)], "<|>")[[1]][3]#
        # Check parent has been processed (has an XML file on which to draw):#
        if(!is.na(match(paste(parentdataset, ".xml", sep = ""), xmlfiles))) {#
            # Isolate taxon names block:#
            taxonnameblock <- currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)]#
            # Reformat as matrix:#
            taxonnameblock <- matrix(unlist(lapply(strsplit(taxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE, dimnames = list(c(), c("ReconName", "ReconNo", "OTUName")))#
            # Add number of OTUs to count:#
            TotalOTUs <- TotalOTUs + nrow(taxonnameblock)#
            # If there are unreconciled taxa:#
            if(any(taxonnameblock[, "ReconNo"] == "-1")) {#
                # Get just the unreconciled names (we don't care about ones already done):#
                unreconcilednames <- taxonnameblock[which(taxonnameblock[, "ReconNo"] == "-1"), "OTUName"]#
                # Read in ith XML file:#
                parentxml <- readLines(paste(parentdataset, ".xml", sep = ""))#
                # Isolate taxon names block:#
                parenttaxonnameblock <- parentxml[(grep("<Taxa", parentxml) + 1):(grep("</Taxa", parentxml) - 1)]#
                # Reformat as matrix:#
                parenttaxonnameblock <- matrix(unlist(lapply(strsplit(parenttaxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE, dimnames = list(c(), c("ReconName", "ReconNo", "OTUName")))#
                # If at least one name can be reconciled using parent data set data:#
                if(any(!is.na(match(unreconcilednames, parenttaxonnameblock[, "OTUName"])))) {#
                    # Update unreconciled names as just those also present in parent data:#
                    unreconcilednames <- unreconcilednames[!is.na(match(unreconcilednames, parenttaxonnameblock[, "OTUName"]))]#
#
                    # Update taxon names block with parent data:#
                    taxonnameblock[match(unreconcilednames, taxonnameblock[, "OTUName"]), ] <- parenttaxonnameblock[match(unreconcilednames, parenttaxonnameblock[, "OTUName"]), ]#
                }#
            }#
            # Add taxonblock back into currentxml:#
            currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)] <- paste("\t\t<List recon_name=\"", taxonnameblock[, "ReconName"], "\" recon_no=\"", taxonnameblock[, "ReconNo"], "\">", taxonnameblock[, "OTUName"], "</List>", sep = "")#
#
        }#
#
    }#
    # Write out XML:#
    write(x = paste(currentxml, collapse = "\n"), file = i)#
}#
#
# Set empty priorites matrix:#
Priorities <- matrix(nrow = 0, ncol = 4, dimnames = list(c(), c("Name", "NUnreconciled", "ParentDepth", "HTMLFile")))#
#
# For each XML file:#
for(i in xmlfiles) {#
  # Read in ith XML file:#
  currentxml <- readLines(i)#
  # Work out which html file the data set belongs to:#
  currenthtml <- names(which(unlist(lapply(lapply(MatrixHTML, grep, pattern = paste("xml/", i, sep = "")), length)) == 1))#
  # Isolate taxon names block:#
  taxonnameblock <- currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)]#
  # Reformat as matrix:#
  taxonnameblock <- matrix(unlist(lapply(strsplit(taxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE, dimnames = list(c(), c("ReconName", "ReconNo", "OTUName")))#
  # Add to priorities matrix:#
  Priorities <- rbind(Priorities, c(i, sum(taxonnameblock[, "ReconNo"] == "-1"), parentdepth[order(parentdepth, decreasing = FALSE)][which(xmlfiles == i)], currenthtml))#
}#
#
# Remove any XMLs where all taxa are already reconciled:#
Priorities <- Priorities[-which(Priorities[, "NUnreconciled"] == "0"), , drop = FALSE]#
#
# Number of files that still need taxa reconciled:#
paste(nrow(Priorities), " files still contain unreconciled taxa (", round(((length(xmlfiles) - nrow(Priorities)) / length(xmlfiles) * 100), 2), "% complete)", sep = "")#
#
# Number of individual OTU names that still need reconciling:#
paste(sum(as.numeric(Priorities[, "NUnreconciled"])), " OTUs are still unreconciled (", round(((TotalOTUs - sum(as.numeric(Priorities[, "NUnreconciled"]))) / TotalOTUs * 100), 2), "% complete)", sep = "")#
#
# Order by number of unreconciled OTUs:#
Priorities <- Priorities[order(as.numeric(Priorities[, "NUnreconciled"])), ]#
#
# Display priorities for archosaurs (excluding Cenozoic birds):#
Priorities[sort(c(which(Priorities[, "HTMLFile"] == "matrarch.html"), which(Priorities[, "HTMLFile"] == "matrdino.html"))), ]
# Load gdata library:#
library(gdata)#
#
# Set working directory:#
setwd("~/Documents/Homepage/www.graemetlloyd.com/xml")#
#
# Get list of xml file names):#
xml.list <- list.files()#
#
# Create empty list to store XML data:#
XML.data <- vector(mode = "character")#
#
# Import XML data and store in list:#
for(i in xml.list) XML.data[i] <- ifelse(length(grep("<Title>", trim(readLines(i)))) > 0, strsplit(trim(readLines)(i)[grep("<Title>", readLines(i))], "<Title>|</Title>")[[1]][2], strsplit(trim(readLines(i))[grep("<Booktitle>", trim(readLines(i)))], "<Booktitle>|</Booktitle>")[[1]][2])#
#
# Set working directory:#
setwd("~/Documents/Homepage/www.graemetlloyd.com/mpts")#
#
# Get list of tree file names (to use later in case of .zip endings):#
trees.list <- list.files()#
#
# Set working directory:#
setwd("~/Documents/Homepage/www.graemetlloyd.com")#
#
# Get file list:#
html.file.list <- list.files()[grep("matr[a-z]{4}.html", list.files())]#
#
# Create empty list to store html:#
html.data <- list()#
#
# For each set of matrices:#
for(i in 1:length(html.file.list)) {#
  # Read in the file:#
  X <- readLines(con = html.file.list[i])#
  # Get beginning sof references:#
  begins <- grep("<p class=\"hangingindent\">", X)#
  # Get endings of references:#
  ends <- grep("</p>", X)#
  # Check p tags are closed:#
  if(length(begins) != length(ends)) stop("Opening and closing paragraph marks do not match up.")#
  # For each reference add html to list (file name, title, actual text block):#
  for(j in 1:length(begins)) html.data[[(length(html.data) + 1)]] <- c(html.file.list[i], trim(X[begins[j] + 1]), X[begins[j]:ends[j]])#
}#
#
# For each html with links:#
for(i in which(unlist(lapply(html.data, length)) > 5)) {#
  # Strip down to just reference:#
  html.data[[i]] <- html.data[[i]][1:5]#
  # Remove remaining beginning of link:#
  html.data[[i]][5] <- gsub("<br><font size=\"-1\">", "</p>", html.data[[i]][5])#
}#
#
# Empty list to store matching titles from XML in HTML:#
matchedhtmldata <- list()#
#
# For each XML file:#
for(i in 1:length(XML.data)) {#
  # Store filename for ith XML file:#
  FileName <- names(XML.data)[i]#
  # Reset i as title sentence:#
  i <- XML.data[i]#
  # Get found matches (grep) for XML title in html data:#
  foundmatches <- grep(i, unlist(lapply(html.data, '[', 2)), fixed = TRUE)#
  # If length of found matcehs is zero (no matches) stop and warn:#
  if(length(foundmatches) == 0) stop("No matches!")#
  # If multiple matches find one closest in character length (most likely outcome!) and update foundmatches accordingly:#
  if(length(foundmatches) > 1) foundmatches <- foundmatches[which(abs(nchar(i) - nchar(unlist(lapply(html.data, '[', 2))[foundmatches])) == min(abs(nchar(i) - nchar(unlist(lapply(html.data, '[', 2))[foundmatches]))))]#
  # If a single match (ideal outcome):#
  if(length(foundmatches) == 1) matchedhtmldata[[(length(matchedhtmldata) + 1)]] <- foundmatches#
  # Final check that found matches do not exceed one:#
  if(length(foundmatches) > 1) {#
    # Get first names of authors on each match:#
    FirstNames <- unlist(lapply(lapply(lapply(lapply(lapply(lapply(lapply(html.data[foundmatches], '[', 3), strsplit, split = "<p class=\"hangingindent\">"), unlist), '[', 2), strsplit, split = ", "), unlist), '[', 1))#
    # Get first name of author from ith XML file:#
    FirstName <- strsplit(strsplit(FileName, split = "[:0-9:]{4}|inpress")[[1]][1], split = "_")[[1]][1]#
    # Update found matches with matching first author surname(s):#
    foundmatches <- foundmatches[which(FirstNames == FirstName)]#
    # If no matches stop and warn user:#
    if(length(foundmatches) == 0) stop("No matches found for first author name.")#
    # If a single match (ideal outcome):#
    if(length(foundmatches) == 1) matchedhtmldata[[(length(matchedhtmldata) + 1)]] <- foundmatches#
    # If multiple matches stop and warn user:#
    if(length(foundmatches) > 1) stop("Multiple matches found for title and first author.")#
  }#
}#
#
# Now only single matches convert matchedhtml to vector:#
matchedhtmldata <- unlist(matchedhtmldata)#
#
# For each HTML file that has an XML file:#
for(i in sort(unique(unlist(matchedhtmldata)))) {#
  # Get filenames for current html matches:#
  filenames <- sort(gsub(".xml", "", names(XML.data[which(matchedhtmldata == i)])))#
  # Create empty vectors to store mpt filenames and treestrings for treevector links:#
  mptsfilenames <- treestrings <- vector(mode = "character")#
  # Set working directory to strict consensus folder:#
  setwd("~/Documents/Homepage/www.graemetlloyd.com/sc")#
  # Get tree Newick strings for treevector links:#
  for(j in paste(filenames, ".tre", sep = "")) treestrings <- c(treestrings, paste("http://supfam.cs.bris.ac.uk/TreeVector/cgi-bin/maketree.cgi?topology=", gsub("\\;", "%3B", gsub("\\)", "%29", gsub("\\(", "%28", readLines(j)))), "&treetype=-clad", sep = ""))#
  # Set working directory to MPTs folder:#
  setwd("~/Documents/Homepage/www.graemetlloyd.com/mpts")#
  # Get MPTs filenames:#
  for(j in filenames) mptsfilenames <- c(mptsfilenames, list.files()[which(lapply(strsplit(list.files(), split = j), '[[', 1) == "")])#
  # Put it all together by updating html.data with links:#
  html.data[[i]] <- gsub("</p>", paste("<br><font size=\"-1\">\n                                        ", paste(paste("<a href=\"nexus/", filenames, ".nex\" target=\"_blank\">NEXUS</a> | <a href=\"tnt/", filenames, ".tnt\" target=\"_blank\">TNT</a> | <a href=\"mpts/", mptsfilenames, "\" target=\"_blank\">MPT(s)</a> <a href=\"firstmpt/", filenames, ".tre\" target=\"_blank\">(1)</a> | <a href=\"sc/", filenames, ".tre\" target=\"_blank\">SC</a> <a href=\"", treestrings, "\" target=\"_blank\">(TV)</a> | <a href=\"mrp/", filenames, "mrp.nex\" target=\"_blank\">MRP</a> | <a href=\"xml/", filenames, ".xml\" target=\"_blank\">XML</a>", sep = ""), collapse = "<br>\n                                    "), "</font></p>", sep = ""), html.data[[i]])#
}#
#
# Set working directory:#
setwd("~/Documents/Homepage/www.graemetlloyd.com")#
#
# For each html file:#
for(i in html.file.list) {#
  # Read in html for ith file:#
  fullhtml <- readLines(i)#
  # Store opening lines:#
  openinglines <- paste(fullhtml[1:max(grep("<a href=\"matr.html\">", fullhtml))], collapse = "\n")#
  # Store closing lines:#
  closinglines <- paste(fullhtml[grep("<!-- InstanceEndEditable -->", fullhtml, fixed = TRUE):length(fullhtml)], collapse = "\n")#
  # Find out which references are on the ith html page:#
  currentrefs <- which(unlist(lapply(html.data, '[', 1)) == i)#
  # Build initial references block (of html code):#
  refsblock <- html.data[currentrefs]#
  # Get publication years for each reference:#
  pubyears <- trim(gsub(".", "", unlist(lapply(lapply(strsplit(unlist(lapply(refsblock, '[', 3)), ","), rev), '[', 1)), fixed = TRUE))#
  # Little check that references are in order:#
  if(any(diff(as.numeric(setdiff(pubyears, "in press"))) > 0)) stop("References out of order!")#
  # Replace lower case in with upper case In for in press stuff:#
  pubyears <- gsub("in press", "In press", pubyears)#
  # Create new refs block (for stroing them collapsed by pub year):#
  newrefsblock <- vector(mode = "character")#
  # For each unique publication year create html block:#
  for(j in unique(pubyears)) newrefsblock <- c(newrefsblock, paste(paste("                                    <h4>", j, "</h4>\n\n", sep = ""), paste(unlist(lapply(lapply(refsblock[which(pubyears == j)], '[', 3:5), paste, collapse = "\n")), collapse = "\n\n"), sep = ""))#
  # Write html to file:#
  write(paste(c(openinglines, newrefsblock, closinglines), collapse = "\n\n"), file = i)#
}
# CODE TO FILL OUT SAFE FIRST GUESSES FOR TAXON RECONCILIATION WHERE MISSING FROM XML FILES#
# I.E., USES EXACT SPECIES NAMES AND PALEOBIOLOGY DATABASE OR USES RECONCILIATION FROM PARENT DATASET FOR EXACT SAME OTU OTHERWISE OTUS ARE NOT ALTERED AT ALL#
#
# Load libraries:#
library(metatree)#
library(Claddis)#
#
# Set working directory to HTML files:#
setwd("~/Documents/Homepage/www.graemetlloyd.com")#
#
# Get list of matrix HTMLs:#
MatrixHTMLfiles <- list.files()[grep("matr[:a-z:]{4}.html", list.files())]#
#
# Create empty HTML list:#
MatrixHTML <- list()#
#
# Read in each raw HTML code into list:#
for(i in 1:length(MatrixHTMLfiles)) MatrixHTML[[i]] <- readLines(MatrixHTMLfiles[i])#
#
# Add file names to HTML list:#
names(MatrixHTML) <- gsub(",html", "", MatrixHTMLfiles)#
#
# Set working directory as XML folder:#
setwd("~/Documents/Homepage/www.graemetlloyd.com/xml")#
#
# List all XML files:#
xmlfiles <- list.files()#
#
# Create empty vector to store parent datasets:#
parentdataset <- vector(mode = "character")#
#
# For each XML file:#
for(i in xmlfiles) {#
    # Read in ith XML file:#
    currentxml <- readLines(i)#
    # If there is a parent data set (from which to draw recinciliation data from):#
    parentdataset <- c(parentdataset, ifelse(length(grep("<Parent>", currentxml)) > 0, strsplit(currentxml[grep("<Parent>", currentxml)], "<|>")[[1]][3], ""))#
}#
#
# Add file names to parent data set vector:#
names(parentdataset) <- gsub(".xml", "", xmlfiles)#
#
# Get vector of "dead" parents, i.e., those not currently present in the data pool:#
deadparents <- sort(setdiff(unique(parentdataset), names(parentdataset)))[which(nchar(sort(setdiff(unique(parentdataset), names(parentdataset)))) > 0)]#
#
# Remove dead parents from parent list:#
for(i in deadparents) parentdataset[which(parentdataset == i)] <- ""#
#
# Create empty parent depth vector (number of links to original data set):#
parentdepth <- vector(mode = "numeric")#
#
# For each data set:#
for(i in names(parentdataset)) {#
    # Set starting depth at zero (no parent at all):#
    currentdepth <- 0#
    # If there is at least an initial parent:#
    if(parentdataset[i] != "") {#
        # Increase depth by one:#
        currentdepth <- currentdepth + 1#
        # While there are further parents:#
        while(parentdataset[i] != "") {#
            # Increase depth by one#
            currentdepth <- currentdepth + 1#
            # Update new parent:#
            i <- parentdataset[i]#
        }#
    }#
    # Add parent depth to vector:#
    parentdepth <- c(parentdepth, currentdepth)#
}#
#
# Reorder xml file lists by parent depth (ensures parents are filled out before children so names can always be carried forwards):#
xmlfiles <- xmlfiles[order(parentdepth, decreasing = FALSE)]#
#
# Count of total OTUs (start with zero):#
TotalOTUs <- 0#
#
# For each XML file:#
for(i in xmlfiles) {#
    # Read in ith XML file:#
    currentxml <- readLines(i)#
    # If there is a parent data set (from which to draw recinciliation data from):#
    if(length(grep("<Parent>", currentxml)) > 0) {#
        # Get parent data set file name:#
        parentdataset <- strsplit(currentxml[grep("<Parent>", currentxml)], "<|>")[[1]][3]#
        # Check parent has been processed (has an XML file on which to draw):#
        if(!is.na(match(paste(parentdataset, ".xml", sep = ""), xmlfiles))) {#
            # Isolate taxon names block:#
            taxonnameblock <- currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)]#
            # Reformat as matrix:#
            taxonnameblock <- matrix(unlist(lapply(strsplit(taxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE, dimnames = list(c(), c("ReconName", "ReconNo", "OTUName")))#
            # Add number of OTUs to count:#
            TotalOTUs <- TotalOTUs + nrow(taxonnameblock)#
            # If there are unreconciled taxa:#
            if(any(taxonnameblock[, "ReconNo"] == "-1")) {#
                # Get just the unreconciled names (we don't care about ones already done):#
                unreconcilednames <- taxonnameblock[which(taxonnameblock[, "ReconNo"] == "-1"), "OTUName"]#
                # Read in ith XML file:#
                parentxml <- readLines(paste(parentdataset, ".xml", sep = ""))#
                # Isolate taxon names block:#
                parenttaxonnameblock <- parentxml[(grep("<Taxa", parentxml) + 1):(grep("</Taxa", parentxml) - 1)]#
                # Reformat as matrix:#
                parenttaxonnameblock <- matrix(unlist(lapply(strsplit(parenttaxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE, dimnames = list(c(), c("ReconName", "ReconNo", "OTUName")))#
                # If at least one name can be reconciled using parent data set data:#
                if(any(!is.na(match(unreconcilednames, parenttaxonnameblock[, "OTUName"])))) {#
                    # Update unreconciled names as just those also present in parent data:#
                    unreconcilednames <- unreconcilednames[!is.na(match(unreconcilednames, parenttaxonnameblock[, "OTUName"]))]#
#
                    # Update taxon names block with parent data:#
                    taxonnameblock[match(unreconcilednames, taxonnameblock[, "OTUName"]), ] <- parenttaxonnameblock[match(unreconcilednames, parenttaxonnameblock[, "OTUName"]), ]#
                }#
            }#
            # Add taxonblock back into currentxml:#
            currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)] <- paste("\t\t<List recon_name=\"", taxonnameblock[, "ReconName"], "\" recon_no=\"", taxonnameblock[, "ReconNo"], "\">", taxonnameblock[, "OTUName"], "</List>", sep = "")#
#
        }#
#
    }#
    # Write out XML:#
    write(x = paste(currentxml, collapse = "\n"), file = i)#
}#
#
# Set empty priorites matrix:#
Priorities <- matrix(nrow = 0, ncol = 4, dimnames = list(c(), c("Name", "NUnreconciled", "ParentDepth", "HTMLFile")))#
#
# For each XML file:#
for(i in xmlfiles) {#
  # Read in ith XML file:#
  currentxml <- readLines(i)#
  # Work out which html file the data set belongs to:#
  currenthtml <- names(which(unlist(lapply(lapply(MatrixHTML, grep, pattern = paste("xml/", i, sep = "")), length)) == 1))#
  # Isolate taxon names block:#
  taxonnameblock <- currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)]#
  # Reformat as matrix:#
  taxonnameblock <- matrix(unlist(lapply(strsplit(taxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE, dimnames = list(c(), c("ReconName", "ReconNo", "OTUName")))#
  # Add to priorities matrix:#
  Priorities <- rbind(Priorities, c(i, sum(taxonnameblock[, "ReconNo"] == "-1"), parentdepth[order(parentdepth, decreasing = FALSE)][which(xmlfiles == i)], currenthtml))#
}#
#
# Remove any XMLs where all taxa are already reconciled:#
Priorities <- Priorities[-which(Priorities[, "NUnreconciled"] == "0"), , drop = FALSE]#
#
# Number of files that still need taxa reconciled:#
paste(nrow(Priorities), " files still contain unreconciled taxa (", round(((length(xmlfiles) - nrow(Priorities)) / length(xmlfiles) * 100), 2), "% complete)", sep = "")#
#
# Number of individual OTU names that still need reconciling:#
paste(sum(as.numeric(Priorities[, "NUnreconciled"])), " OTUs are still unreconciled (", round(((TotalOTUs - sum(as.numeric(Priorities[, "NUnreconciled"]))) / TotalOTUs * 100), 2), "% complete)", sep = "")#
#
# Order by number of unreconciled OTUs:#
Priorities <- Priorities[order(as.numeric(Priorities[, "NUnreconciled"])), ]#
#
# Display priorities for archosaurs (excluding Cenozoic birds):#
Priorities[sort(c(which(Priorities[, "HTMLFile"] == "matrarch.html"), which(Priorities[, "HTMLFile"] == "matrdino.html"))), ]
system(ls)
system("ls")
system("tnt")
system("quit;")
system("ls")
system("cd ~/")
system("ls")
system("cd")
system("ls")
system("tnt; run ~/Hartman_etal_2019a.tnt;")
system("view;")
system("cd ~)
system("cd ~")
system("ls")
system("cd ~")
system("cd /Users/eargtl")
system("ls")
system("cd /Users/eargtl; ls")
setwd("~")
list.files()
system("/Users/eargtl/runalltnt.sh")
# R script to perform strat congruence tests.#
#
# Load strap libraries:#
library(strap)#
#
setwd("~/Documents/Publications/in prep/Strat congruence - April/ProjectWhalehead/Data/Ages")#
#
list.files()
# R script to perform strat congruence tests.#
#
# Load strap libraries:#
library(strap)#
#
setwd("~/Documents/Publications/in prep/Strat congruence - April/ProjectWhalehead/Data/Ages")#
#
list.files()gsub(".txt", "", list.files(), fixed = TRUE)
gsub(".txt", "", list.files(), fixed = TRUE)
paste("~/Documents/Publications/in prep/Strat congruence - April/ProjectWhalehead/Data/MPTs/", DataSets, ".tre", sep = "")
# R script to perform strat congruence tests.#
#
# Load strap libraries:#
library(strap)#
#
# Set working directory:#
setwd("~/Documents/Publications/in prep/Strat congruence - April/ProjectWhalehead/Data/Ages")#
#
##
DataSets <- gsub(".txt", "", list.files(), fixed = TRUE)#
#
paste("~/Documents/Publications/in prep/Strat congruence - April/ProjectWhalehead/Data/MPTs/", DataSets, ".tre", sep = "")
# R script to perform strat congruence tests.#
#
# Load strap libraries:#
library(strap)#
#
# Set working directory:#
setwd("~/Documents/Publications/in prep/Strat congruence - April/ProjectWhalehead/Data/Ages")#
#
##
DataSets <- gsub(".txt", "", list.files(), fixed = TRUE)#
AgePaths <- paste("~/Documents/Publications/in prep/Strat congruence - April/ProjectWhalehead/Data/Ages/", DataSets, ".txt", sep = "")#
#
MPTPaths <- paste("~/Documents/Publications/in prep/Strat congruence - April/ProjectWhalehead/Data/MPTs/", DataSets, ".tre", sep = "")#
#
for(i in 1:length(DataSets)) {#
  read.table(AgePaths)#
}
for(i in 1:length(DataSets)) {#
  read.table(AgePaths[i])#
}
read.table(AgePaths[i])
read.table(AgePaths[i], header = TRUE)
ape::read.tree(MPTPaths[1])
Trees <- ape::read.tree(MPTPaths[1])
class(Trees)
class(Trees) == "phylo"
Trees <- ape::read.tree(MPTPaths[1])#
  if(class(Trees) == "phylo") {#
    Trees <- list(Trees)#
    class(Trees) <- "multiPhylo"#
  }
Trees
?strap
?strap
RawAges <- read.table(AgePaths[i], header = TRUE)#
  Trees <- ape::read.tree(MPTPaths[1])#
  if(class(Trees) == "phylo") {#
    Trees <- list(Trees)#
    class(Trees) <- "multiPhylo"#
  }#
  StratPhyloCongruence(trees = Trees, ages = RawAges, rlen = 0, method = "basic", samp.perm = 1000, rand.perm = 1000, hard = TRUE, randomly.sample.ages = FALSE, fix.topology = FALSE, fix.outgroup = FALSE)
RawAges
RawAges[, "Taxon"]
unique(RawAges[, "Taxon"])
read.table(AgePaths[i], header = TRUE)
?read.table
read.table(AgePaths[i], header = TRUE, colClasses = "character")
read.table(AgePaths[i], header = TRUE, colClasses = "character", as.is = c(TRUE, TRUE, TRUE))
read.table(AgePaths[i], header = TRUE, colClasses = "character", as.is = c(TRUE, TRUE, TRUE), stringsAsFactors = FALSE)
RawAges <- read.table(AgePaths[i], header = TRUE)#
  Trees <- ape::read.tree(MPTPaths[1])#
  if(class(Trees) == "phylo") {#
    Trees <- list(Trees)#
    class(Trees) <- "multiPhylo"#
  }
RawAges
read.table(AgePaths[i], header = TRUE, colClasses = "character", as.is = c(TRUE, TRUE, TRUE), stringsAsFactors = FALSE)
read.table(AgePaths[i], header = TRUE, colClasses = "character", as.is = c(TRUE, TRUE, TRUE), stringsAsFactors = c(FALSE, FALSE, FALSE))
read.table(AgePaths[i], header = TRUE, colClasses = "character", as.is = c(TRUE, TRUE, TRUE), stringsAsFactors = c(FALSE, FALSE, FALSE))[, 1]
read.table(AgePaths[i], header = TRUE, colClasses = "character", as.is = c(TRUE, TRUE, TRUE), stringsAsFactors = c(FALSE, FALSE, FALSE))
read.table(AgePaths[i], header = TRUE, colClasses = "character", as.is = c(TRUE, TRUE, TRUE), stringsAsFactors = c(FALSE, FALSE, FALSE))[, 1]
read.table(AgePaths[i], header = TRUE, colClasses = "character", tringsAsFactors = c(FALSE, FALSE, FALSE))[, 1]
read.table(AgePaths[i], header = TRUE, colClasses = "character", stringsAsFactors = c(FALSE, FALSE, FALSE))[, 1]
read.table(AgePaths[i], header = TRUE, colClasses = "character", stringsAsFactors = FALSE)[, 1]
read.table(AgePaths[i], header = TRUE, stringsAsFactors = FALSE)[, 1]
RawAges <- read.table(AgePaths[i], header = TRUE, stringsAsFactors = FALSE)#
  Trees <- ape::read.tree(MPTPaths[1])#
  if(class(Trees) == "phylo") {#
    Trees <- list(Trees)#
    class(Trees) <- "multiPhylo"#
  }
Trees
?StratPhyloCongruence
RawAges
RawAges[, "taxon"]
RawAges[, "Taxon"]
unique(RawAges[, "Taxon"])
as.list(unique(RawAges[, "Taxon"]))
lapply(as.list(unique(RawAges[, "Taxon"])), function(x) RawAges[RawAges[, "Taxon"] == x, ])
lapply(as.list(unique(RawAges[, "Taxon"])), function(x) apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, sep = "-"))
lapply(as.list(unique(RawAges[, "Taxon"])), function(x) apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, collapse = "-"))
lapply(as.list(unique(RawAges[, "Taxon"])), function(x) unique(apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, collapse = "-")))
lapply(as.list(unique(RawAges[, "Taxon"])), function(x) {y <- unique(apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, collapse = "-")); y})
lapply(as.list(unique(RawAges[, "Taxon"])), function(x) {y <- unique(apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, collapse = "-")); strsplit(y, "-")})
lapply(as.list(unique(RawAges[, "Taxon"])), function(x) {y <- unique(apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, collapse = "-")); do.call(rbind, strsplit(y, "-"))})
lapply(as.list(unique(RawAges[, "Taxon"])), function(x) {y <- unique(apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, collapse = "-")); y <- do.call(rbind, strsplit(y, "-")); y[, 2]})
lapply(as.list(unique(RawAges[, "Taxon"])), function(x) {y <- unique(apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, collapse = "-")); y <- do.call(rbind, strsplit(y, "-")); LAD <- max(as.numeric(y[, 2]))})
lapply(as.list(unique(RawAges[, "Taxon"])), function(x) {y <- unique(apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, collapse = "-")); y <- do.call(rbind, strsplit(y, "-")); LAD <- max(as.numeric(y[, 2])); LAD})
lapply(as.list(unique(RawAges[, "Taxon"])), function(x) {y <- unique(apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, collapse = "-")); y <- do.call(rbind, strsplit(y, "-")); LAD <- max(as.numeric(y[, 2])); as.numeric(y[, 1])})
lapply(as.list(unique(RawAges[, "Taxon"])), function(x) {y <- unique(apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, collapse = "-")); y <- do.call(rbind, strsplit(y, "-")); LAD <- max(as.numeric(y[, 2])); FADs <- as.numeric(y[, 1]); FADs[FADs > LAD]})
lapply(as.list(unique(RawAges[, "Taxon"])), function(x) {y <- unique(apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, collapse = "-")); y <- do.call(rbind, strsplit(y, "-")); LAD <- max(as.numeric(y[, 2])); FADs <- as.numeric(y[, 1]); FAD <- min(FADs[FADs >= LAD]); c(FAD, LAD)})
do.call(rbind, lapply(as.list(unique(RawAges[, "Taxon"])), function(x) {y <- unique(apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, collapse = "-")); y <- do.call(rbind, strsplit(y, "-")); LAD <- max(as.numeric(y[, 2])); FADs <- as.numeric(y[, 1]); FAD <- min(FADs[FADs >= LAD]); c(FAD, LAD)}))
RawAges <- read.table(AgePaths[i], header = TRUE, stringsAsFactors = FALSE)#
  TipAges <- do.call(rbind, lapply(as.list(unique(RawAges[, "Taxon"])), function(x) {y <- unique(apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, collapse = "-")); y <- do.call(rbind, strsplit(y, "-")); LAD <- max(as.numeric(y[, 2])); FADs <- as.numeric(y[, 1]); FAD <- min(FADs[FADs >= LAD]); c(FAD, LAD)}))#
  rownames(TipAges) <- unique(RawAges[, "Taxon"])#
  colnames(TipAges) <- c("FAD", "LAD")#
  Trees <- ape::read.tree(MPTPaths[1])#
  if(class(Trees) == "phylo") {#
    Trees <- list(Trees)#
    class(Trees) <- "multiPhylo"#
  }#
  StratPhyloCongruence(trees = Trees, ages = TipAges, rlen = 0, method = "basic", samp.perm = 1000, rand.perm = 1000, hard = TRUE, randomly.sample.ages = FALSE, fix.topology = FALSE, fix.outgroup = FALSE)
Trees
Trees[[1]]
Trees[[1]]Trees[[1]]$tip.label
Trees[[1]]$tip.label
TipAges
for(i in 1:length(DataSets)) {#
}
i
RawAges <- read.table(AgePaths[i], header = TRUE, stringsAsFactors = FALSE)#
  TipAges <- do.call(rbind, lapply(as.list(unique(RawAges[, "Taxon"])), function(x) {y <- unique(apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, collapse = "-")); y <- do.call(rbind, strsplit(y, "-")); LAD <- max(as.numeric(y[, 2])); FADs <- as.numeric(y[, 1]); FAD <- min(FADs[FADs >= LAD]); c(FAD, LAD)}))#
  rownames(TipAges) <- unique(RawAges[, "Taxon"])#
  colnames(TipAges) <- c("FAD", "LAD")#
  Trees <- ape::read.tree(MPTPaths[i])#
  if(class(Trees) == "phylo") {#
    Trees <- list(Trees)#
    class(Trees) <- "multiPhylo"#
  }#
  StratPhyloCongruence(trees = Trees, ages = TipAges, rlen = 0, method = "basic", samp.perm = 1000, rand.perm = 1000, hard = TRUE, randomly.sample.ages = FALSE, fix.topology = FALSE, fix.outgroup = FALSE)
for(i in 1:length(DataSets)) {#
  RawAges <- read.table(AgePaths[i], header = TRUE, stringsAsFactors = FALSE)#
  TipAges <- do.call(rbind, lapply(as.list(unique(RawAges[, "Taxon"])), function(x) {y <- unique(apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, collapse = "-")); y <- do.call(rbind, strsplit(y, "-")); LAD <- max(as.numeric(y[, 2])); FADs <- as.numeric(y[, 1]); FAD <- min(FADs[FADs >= LAD]); c(FAD, LAD)}))#
  rownames(TipAges) <- unique(RawAges[, "Taxon"])#
  colnames(TipAges) <- c("FAD", "LAD")#
  Trees <- ape::read.tree(MPTPaths[i])#
  if(class(Trees) == "phylo") {#
    Trees <- list(Trees)#
    class(Trees) <- "multiPhylo"#
  }#
  StratPhyloCongruenceResults <- StratPhyloCongruence(trees = Trees, ages = TipAges, rlen = 0, method = "basic", samp.perm = 1000, rand.perm = 1000, hard = TRUE, randomly.sample.ages = FALSE, fix.topology = FALSE, fix.outgroup = FALSE)#
}
i
RawAges <- read.table(AgePaths[i], header = TRUE, stringsAsFactors = FALSE)#
  TipAges <- do.call(rbind, lapply(as.list(unique(RawAges[, "Taxon"])), function(x) {y <- unique(apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, collapse = "-")); y <- do.call(rbind, strsplit(y, "-")); LAD <- max(as.numeric(y[, 2])); FADs <- as.numeric(y[, 1]); FAD <- min(FADs[FADs >= LAD]); c(FAD, LAD)}))#
  rownames(TipAges) <- unique(RawAges[, "Taxon"])#
  colnames(TipAges) <- c("FAD", "LAD")#
  Trees <- ape::read.tree(MPTPaths[i])#
  if(class(Trees) == "phylo") {#
    Trees <- list(Trees)#
    class(Trees) <- "multiPhylo"#
  }#
  StratPhyloCongruenceResults <- StratPhyloCongruence(trees = Trees, ages = TipAges, rlen = 0, method = "basic", samp.perm = 1000, rand.perm = 1000, hard = TRUE, randomly.sample.ages = FALSE, fix.topology = FALSE, fix.outgroup = FALSE)
Trees
Trees[[1]]$tip.label
TipAges
rownames(TipAges)
Trees[[1]]$tip.label
unique(Trees[[1]]$tip.label)
sort(Trees[[1]]$tip.label)
sort(rownames(TipAges))
?pcoa
?Claddis
library(Claddis)
MorphDistMatrix(Michaux1989)
MorphDistMatrix(Michaux1989)$DistanceMatrix
pcoa(MorphDistMatrix(Michaux1989)$DistanceMatrix)
RawAges <- read.table(AgePaths[i], header = TRUE, stringsAsFactors = FALSE)#
  TipAges <- do.call(rbind, lapply(as.list(unique(RawAges[, "Taxon"])), function(x) {y <- unique(apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, collapse = "-")); y <- do.call(rbind, strsplit(y, "-")); LAD <- max(as.numeric(y[, 2])); FADs <- as.numeric(y[, 1]); FAD <- min(FADs[FADs >= LAD]); c(FAD, LAD)}))#
  rownames(TipAges) <- unique(RawAges[, "Taxon"])#
  colnames(TipAges) <- c("FAD", "LAD")#
  Trees <- ape::read.tree(MPTPaths[i])#
  if(class(Trees) == "phylo") {#
    Trees <- list(Trees)#
    class(Trees) <- "multiPhylo"#
  }#
  StratPhyloCongruenceResults <- StratPhyloCongruence(trees = Trees, ages = TipAges, rlen = 0, method = "basic", samp.perm = 1000, rand.perm = 1000, hard = TRUE, randomly.sample.ages = FALSE, fix.topology = FALSE, fix.outgroup = FALSE)
DataSets[i]
sort(Trees[[1]]$tip.label)#
  sort(rownames(TipAges))
apply(cbind(sort(rownames(TipAges)), sort(Trees[[1]]$tip.label)), 1, as.list)
lapply(apply(cbind(sort(rownames(TipAges)), sort(Trees[[1]]$tip.label)), 1, as.list), function(x) unlist(x))
cbind(sort(rownames(TipAges)), sort(Trees[[1]]$tip.label))
setdiff(sort(rownames(TipAges)), sort(Trees[[1]]$tip.label))
setdiff(sort(rownames(TipAges)), sort(Trees[[1]]$tip.label))
# R script to perform strat congruence tests.#
#
# Load strap libraries:#
library(strap)#
#
# Set working directory:#
setwd("~/Documents/Publications/in prep/Strat congruence - April/ProjectWhalehead/Data/Ages")#
#
##
DataSets <- gsub(".txt", "", list.files(), fixed = TRUE)#
AgePaths <- paste("~/Documents/Publications/in prep/Strat congruence - April/ProjectWhalehead/Data/Ages/", DataSets, ".txt", sep = "")#
#
MPTPaths <- paste("~/Documents/Publications/in prep/Strat congruence - April/ProjectWhalehead/Data/MPTs/", DataSets, ".tre", sep = "")#
#
for(i in 1:length(DataSets)) {#
  RawAges <- read.table(AgePaths[i], header = TRUE, stringsAsFactors = FALSE)#
  TipAges <- do.call(rbind, lapply(as.list(unique(RawAges[, "Taxon"])), function(x) {y <- unique(apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, collapse = "-")); y <- do.call(rbind, strsplit(y, "-")); LAD <- max(as.numeric(y[, 2])); FADs <- as.numeric(y[, 1]); FAD <- min(FADs[FADs >= LAD]); c(FAD, LAD)}))#
  rownames(TipAges) <- unique(RawAges[, "Taxon"])#
  colnames(TipAges) <- c("FAD", "LAD")#
  Trees <- ape::read.tree(MPTPaths[i])#
  if(class(Trees) == "phylo") {#
    Trees <- list(Trees)#
    class(Trees) <- "multiPhylo"#
  }#
  # If tip names and age names do not match:#
  if(length(setdiff(sort(rownames(TipAges)), sort(Trees[[1]]$tip.label))) > 0) {#
    lapply(apply(cbind(sort(rownames(TipAges)), sort(Trees[[1]]$tip.label)), 1, as.list), function(x) unlist(x))#
    cat(DataSets[i], " ")#
  }}
for(i in 1:length(DataSets)) {#
  RawAges <- read.table(AgePaths[i], header = TRUE, stringsAsFactors = FALSE)#
  TipAges <- do.call(rbind, lapply(as.list(unique(RawAges[, "Taxon"])), function(x) {y <- unique(apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, collapse = "-")); y <- do.call(rbind, strsplit(y, "-")); LAD <- max(as.numeric(y[, 2])); FADs <- as.numeric(y[, 1]); FAD <- min(FADs[FADs >= LAD]); c(FAD, LAD)}))#
  rownames(TipAges) <- unique(RawAges[, "Taxon"])#
  colnames(TipAges) <- c("FAD", "LAD")#
  Trees <- ape::read.tree(MPTPaths[i])#
  if(class(Trees) == "phylo") {#
    Trees <- list(Trees)#
    class(Trees) <- "multiPhylo"#
  }#
  # If tip names and age names do not match:#
  if(length(setdiff(sort(rownames(TipAges)), sort(Trees[[1]]$tip.label))) > 0) {#
    lapply(apply(cbind(sort(rownames(TipAges)), sort(Trees[[1]]$tip.label)), 1, as.list), function(x) unlist(x))#
    stop()#
  }#
  }
lapply(apply(cbind(sort(rownames(TipAges)), sort(Trees[[1]]$tip.label)), 1, as.list), function(x) unlist(x))
do.call(rbind, lapply(apply(cbind(sort(rownames(TipAges)), sort(Trees[[1]]$tip.label)), 1, as.list), function(x) unlist(x)))
do.call(rbind, lapply(apply(cbind(sort(rownames(TipAges)), sort(Trees[[1]]$tip.label)), 1, as.list), function(x) unlist(x)))
NameMatches <- do.call(rbind, lapply(apply(cbind(sort(rownames(TipAges)), sort(Trees[[1]]$tip.label)), 1, as.list), function(x) unlist(x)))
NameMatches
NameMatches[, 1] != NameMatches [ , 2]
NameMatches[NameMatches[, 1] != NameMatches [ , 2], ]
NameMatches[NameMatches[, 1] != NameMatches [ , 2], , drop = FALSE]
lapply(Trees, function(x) x$tip.label)
lapply(Trees, function(x) x$tip.label == NameMatches [j, 2])
j<-1
lapply(Trees, function(x) x$tip.label == NameMatches [j, 2])
lapply(Trees, function(x) x$tip.label[x$tip.label == NameMatches [j, 2]])
lapply(Trees, function(x) {x$tip.label[x$tip.label == NameMatches [j, 2]] <- NameMatches [j, 1]; x})
# R script to perform strat congruence tests.#
#
# Load strap libraries:#
library(strap)#
#
# Set working directory:#
setwd("~/Documents/Publications/in prep/Strat congruence - April/ProjectWhalehead/Data/Ages")#
#
##
DataSets <- gsub(".txt", "", list.files(), fixed = TRUE)#
AgePaths <- paste("~/Documents/Publications/in prep/Strat congruence - April/ProjectWhalehead/Data/Ages/", DataSets, ".txt", sep = "")#
#
MPTPaths <- paste("~/Documents/Publications/in prep/Strat congruence - April/ProjectWhalehead/Data/MPTs/", DataSets, ".tre", sep = "")#
#
for(i in 1:length(DataSets)) {#
  RawAges <- read.table(AgePaths[i], header = TRUE, stringsAsFactors = FALSE)#
  TipAges <- do.call(rbind, lapply(as.list(unique(RawAges[, "Taxon"])), function(x) {y <- unique(apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, collapse = "-")); y <- do.call(rbind, strsplit(y, "-")); LAD <- max(as.numeric(y[, 2])); FADs <- as.numeric(y[, 1]); FAD <- min(FADs[FADs >= LAD]); c(FAD, LAD)}))#
  rownames(TipAges) <- unique(RawAges[, "Taxon"])#
  colnames(TipAges) <- c("FAD", "LAD")#
  Trees <- ape::read.tree(MPTPaths[i])#
  if(class(Trees) == "phylo") {#
    Trees <- list(Trees)#
    class(Trees) <- "multiPhylo"#
  }#
  # If tip names and age names do not match:#
  if(length(setdiff(sort(rownames(TipAges)), sort(Trees[[1]]$tip.label))) > 0) {#
    NameMatches <- do.call(rbind, lapply(apply(cbind(sort(rownames(TipAges)), sort(Trees[[1]]$tip.label)), 1, as.list), function(x) unlist(x)))#
    NameMatches <- NameMatches[NameMatches[, 1] != NameMatches [ , 2], , drop = FALSE]#
    for(j in 1:nrow(NameMatches)) {#
      Trees <- lapply(Trees, function(x) {x$tip.label[x$tip.label == NameMatches [j, 2]] <- NameMatches [j, 1]; x})#
      class(Trees) <- "multiPhylo"#
    }#
  }}
Trees
StratPhyloCongruenceResults <- StratPhyloCongruence(trees = Trees, ages = TipAges, rlen = 0, method = "basic", samp.perm = 1000, rand.perm = 1000, hard = TRUE, randomly.sample.ages = FALSE, fix.topology = FALSE, fix.outgroup = FALSE)
names(StratPhyloCongruenceResults)
StratPhyloCongruenceResults[[1]]
StratPhyloCongruenceResults[[2]]
StratPhyloCongruenceResults[[3]]
StratPhyloCongruenceResults[[3]][1:2, ]
StratPhyloCongruenceResults[[3]][, "MIG"]
plot(StratPhyloCongruenceResults[[3]][, "MIG"])
StratPhyloCongruenceResults <- StratPhyloCongruence(trees = Trees, ages = TipAges, rlen = 0, method = "basic", samp.perm = 1000, rand.perm = 1000, hard = TRUE, randomly.sample.ages = TRUE, fix.topology = FALSE, fix.outgroup = FALSE)
plot(StratPhyloCongruenceResults[[3]][, "MIG"])
?metatree
library(metatree)
?metatree
plot(StratPhyloCongruenceResults[[1]]
)
StratPhyloCongruenceResults[[1]]
StratPhyloCongruenceResults[[2]]
names(StratPhyloCongruenceResults)
StratPhyloCongruenceResults$rand.trees
c(Trees, StratPhyloCongruenceResults$rand.trees)
?metatree
TreePool <- c(Trees, StratPhyloCongruenceResults$rand.trees)#
  ContradictionMatrix <- MultiTreeContradiction(trees = TreePool, rescale = TRUE)
ContradictionMatrix[1:2, 1:2]
PCoA <- ape::pcoa(ContradictionMatrix, k = 2)
?pcoa
PCoA <- ape::pcoa(ContradictionMatrix)
names(PCoA)
PCoA$vectors[1:2, ]
plot(PCoA$vectors[, 1], PCoA$vectors[, 2])
library(hypRspace)
?hypRspace
names(StratPhyloCongruenceResults)
StratPhyloCongruenceResults$input.tree.results[, "MIG"]
StratPhyloCongruenceResults$rand.permutations[, "MIG"]
CircularVoronoi(x = PCoA$vectors[, 1], y = PCoA$vectors[, 2], heat = MIGs, EdgeFactor = 1.1, NColours = 100)
MIGs <- c(StratPhyloCongruenceResults$input.tree.results[, "MIG"], StratPhyloCongruenceResults$rand.permutations[, "MIG"])#
  CircularVoronoi(x = PCoA$vectors[, 1], y = PCoA$vectors[, 2], heat = MIGs, EdgeFactor = 1.1, NColours = 100)
MIGs <- c(StratPhyloCongruenceResults$input.tree.results[, "MIG"], StratPhyloCongruenceResults$rand.permutations[, "MIG"])#
  CircularVoronoi(x = PCoA$vectors[, 1], y = PCoA$vectors[, 2], heat = (MIGs / max(MIGs)), EdgeFactor = 1.1, NColours = 100)
(MIGs / max(MIGs))
MIGs - min(MIG)
MIGs - min(MIGs)
(MIGs - min(MIGs)) / max(MIGs - min(MIGs))
CircularVoronoi(x = PCoA$vectors[, 1], y = PCoA$vectors[, 2], heat = (MIGs - min(MIGs)) / max(MIGs - min(MIGs)), EdgeFactor = 1.1, NColours = 100)
paste("~/Documents/Publications/in prep/Strat congruence - April/ProjectWhalehead/Data/Sample Plots", DataSets[i])
paste("~/Documents/Publications/in prep/Strat congruence - April/ProjectWhalehead/Data/Sample Plots/", DataSets[i], ".pdf", sep = "")
# R script to perform strat congruence tests.#
#
# Load metatree and strap libraries:#
library(hypSpace)#
library(metatree)#
library(strap)#
#
# Set working directory:#
setwd("~/Documents/Publications/in prep/Strat congruence - April/ProjectWhalehead/Data/Ages")#
#
##
DataSets <- gsub(".txt", "", list.files(), fixed = TRUE)#
AgePaths <- paste("~/Documents/Publications/in prep/Strat congruence - April/ProjectWhalehead/Data/Ages/", DataSets, ".txt", sep = "")#
#
MPTPaths <- paste("~/Documents/Publications/in prep/Strat congruence - April/ProjectWhalehead/Data/MPTs/", DataSets, ".tre", sep = "")#
#
for(i in 1:length(DataSets)) {#
  RawAges <- read.table(AgePaths[i], header = TRUE, stringsAsFactors = FALSE)#
  TipAges <- do.call(rbind, lapply(as.list(unique(RawAges[, "Taxon"])), function(x) {y <- unique(apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, collapse = "-")); y <- do.call(rbind, strsplit(y, "-")); LAD <- max(as.numeric(y[, 2])); FADs <- as.numeric(y[, 1]); FAD <- min(FADs[FADs >= LAD]); c(FAD, LAD)}))#
  rownames(TipAges) <- unique(RawAges[, "Taxon"])#
  colnames(TipAges) <- c("FAD", "LAD")#
  Trees <- ape::read.tree(MPTPaths[i])#
  if(class(Trees) == "phylo") {#
    Trees <- list(Trees)#
    class(Trees) <- "multiPhylo"#
  }#
  # If tip names and age names do not match:#
  if(length(setdiff(sort(rownames(TipAges)), sort(Trees[[1]]$tip.label))) > 0) {#
    NameMatches <- do.call(rbind, lapply(apply(cbind(sort(rownames(TipAges)), sort(Trees[[1]]$tip.label)), 1, as.list), function(x) unlist(x)))#
    NameMatches <- NameMatches[NameMatches[, 1] != NameMatches [ , 2], , drop = FALSE]#
    for(j in 1:nrow(NameMatches)) {#
      Trees <- lapply(Trees, function(x) {x$tip.label[x$tip.label == NameMatches [j, 2]] <- NameMatches [j, 1]; x})#
      class(Trees) <- "multiPhylo"#
    }#
  }#
  StratPhyloCongruenceResults <- StratPhyloCongruence(trees = Trees, ages = TipAges, rlen = 0, method = "basic", samp.perm = 1000, rand.perm = 1000, hard = TRUE, randomly.sample.ages = TRUE, fix.topology = FALSE, fix.outgroup = FALSE)#
  TreePool <- c(Trees, StratPhyloCongruenceResults$rand.trees)#
  ContradictionMatrix <- MultiTreeContradiction(trees = TreePool, rescale = TRUE)#
  PCoA <- ape::pcoa(ContradictionMatrix)#
  MIGs <- c(StratPhyloCongruenceResults$input.tree.results[, "MIG"], StratPhyloCongruenceResults$rand.permutations[, "MIG"])#
  pdf(paste("~/Documents/Publications/in prep/Strat congruence - April/ProjectWhalehead/Data/Sample Plots/", DataSets[i], ".pdf", sep = ""))#
  CircularVoronoi(x = PCoA$vectors[, 1], y = PCoA$vectors[, 2], heat = (MIGs - min(MIGs)) / max(MIGs - min(MIGs)), EdgeFactor = 1.1, NColours = 100)#
  dev.off()#
}
PCoA
# Build PCoA from contradiction matrix:#
  PCoA <- ape::pcoa(ContradictionMatrix)
TreePool
plot(ContradictionMatrix)
unique(ContradictionMatrix)
unique(as.vector(ContradictionMatrix))
PCoA <- ape::pcoa(as.dist(ContradictionMatrix))
TreePool[[1]]
plot(TreePool[[1]])
i
RawAges <- read.table(AgePaths[i], header = TRUE, stringsAsFactors = FALSE)#
  TipAges <- do.call(rbind, lapply(as.list(unique(RawAges[, "Taxon"])), function(x) {y <- unique(apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, collapse = "-")); y <- do.call(rbind, strsplit(y, "-")); LAD <- max(as.numeric(y[, 2])); FADs <- as.numeric(y[, 1]); FAD <- min(FADs[FADs >= LAD]); c(FAD, LAD)}))#
  rownames(TipAges) <- unique(RawAges[, "Taxon"])#
  colnames(TipAges) <- c("FAD", "LAD")#
  Trees <- ape::read.tree(MPTPaths[i])#
  if(class(Trees) == "phylo") {#
    Trees <- list(Trees)#
    class(Trees) <- "multiPhylo"#
  }#
  # If tip names and age names do not match:#
  if(length(setdiff(sort(rownames(TipAges)), sort(Trees[[1]]$tip.label))) > 0) {#
    NameMatches <- do.call(rbind, lapply(apply(cbind(sort(rownames(TipAges)), sort(Trees[[1]]$tip.label)), 1, as.list), function(x) unlist(x)))#
    NameMatches <- NameMatches[NameMatches[, 1] != NameMatches [ , 2], , drop = FALSE]#
    for(j in 1:nrow(NameMatches)) {#
      Trees <- lapply(Trees, function(x) {x$tip.label[x$tip.label == NameMatches [j, 2]] <- NameMatches [j, 1]; x})#
      class(Trees) <- "multiPhylo"#
    }#
  }#
  # Get strat congruence results:#
  StratPhyloCongruenceResults <- StratPhyloCongruence(trees = Trees, ages = TipAges, rlen = 0, method = "basic", samp.perm = 1000, rand.perm = 1000, hard = TRUE, randomly.sample.ages = TRUE, fix.topology = FALSE, fix.outgroup = FALSE)#
  # Pool sampled and raondomly generated trees tgether:#
  TreePool <- c(Trees, StratPhyloCongruenceResults$rand.trees)#
  # Pool minimum implied gaps for these trees together:#
  MIGs <- c(StratPhyloCongruenceResults$input.tree.results[, "MIG"], StratPhyloCongruenceResults$rand.permutations[, "MIG"])#
#
  # Get tree contradiction matrix:#
  ContradictionMatrix <- MultiTreeContradiction(trees = TreePool, rescale = TRUE)
# Build PCoA from contradiction matrix:#
  PCoA <- ape::pcoa(as.dist(ContradictionMatrix))
nrow(ContradictionMatrix)
ncol(ContradictionMatrix)
?metatree
RFMatrix <- MultiTreeDistance(trees = TreePool, distance = "RF", scale = TRUE)
# Build PCoA from contradiction matrix:#
PCoA <- ape::pcoa(as.dist(RFMatrix))
ncol(RFMatrix)
nrow(RFMatrix)
as.vector(RFMatrix)
unique(as.vector(RFMatrix))
hist(as.vector(RFMatrix), breaks = 50)
?asin
# Build PCoA from contradiction matrix:#
  PCoA <- ape::pcoa(asin(sqrt(as.dist(RFMatrix))))
CircularVoronoi(x = PCoA$vectors[, 1], y = PCoA$vectors[, 2], heat = (MIGs - min(MIGs)) / max(MIGs - min(MIGs)), EdgeFactor = 1.1, NColours = 100)
# R script to perform strat congruence tests.#
#
# Load metatree and strap libraries:#
library(hypSpace)#
library(metatree)#
library(strap)#
#
# Set working directory:#
setwd("~/Documents/Publications/in prep/Strat congruence - April/ProjectWhalehead/Data/Ages")#
#
# Build vector of data sets:#
DataSets <- gsub(".txt", "", list.files(), fixed = TRUE)#
AgePaths <- paste("~/Documents/Publications/in prep/Strat congruence - April/ProjectWhalehead/Data/Ages/", DataSets, ".txt", sep = "")#
#
MPTPaths <- paste("~/Documents/Publications/in prep/Strat congruence - April/ProjectWhalehead/Data/MPTs/", DataSets, ".tre", sep = "")#
#
for(i in 1:length(DataSets)) {#
  RawAges <- read.table(AgePaths[i], header = TRUE, stringsAsFactors = FALSE)#
  TipAges <- do.call(rbind, lapply(as.list(unique(RawAges[, "Taxon"])), function(x) {y <- unique(apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, collapse = "-")); y <- do.call(rbind, strsplit(y, "-")); LAD <- max(as.numeric(y[, 2])); FADs <- as.numeric(y[, 1]); FAD <- min(FADs[FADs >= LAD]); c(FAD, LAD)}))#
  rownames(TipAges) <- unique(RawAges[, "Taxon"])#
  colnames(TipAges) <- c("FAD", "LAD")#
  Trees <- ape::read.tree(MPTPaths[i])#
  if(class(Trees) == "phylo") {#
    Trees <- list(Trees)#
    class(Trees) <- "multiPhylo"#
  }#
  # If tip names and age names do not match:#
  if(length(setdiff(sort(rownames(TipAges)), sort(Trees[[1]]$tip.label))) > 0) {#
    NameMatches <- do.call(rbind, lapply(apply(cbind(sort(rownames(TipAges)), sort(Trees[[1]]$tip.label)), 1, as.list), function(x) unlist(x)))#
    NameMatches <- NameMatches[NameMatches[, 1] != NameMatches [ , 2], , drop = FALSE]#
    for(j in 1:nrow(NameMatches)) {#
      Trees <- lapply(Trees, function(x) {x$tip.label[x$tip.label == NameMatches [j, 2]] <- NameMatches [j, 1]; x})#
      class(Trees) <- "multiPhylo"#
    }#
  }#
  # Get strat congruence results:#
  StratPhyloCongruenceResults <- StratPhyloCongruence(trees = Trees, ages = TipAges, rlen = 0, method = "basic", samp.perm = 1000, rand.perm = 1000, hard = TRUE, randomly.sample.ages = TRUE, fix.topology = FALSE, fix.outgroup = FALSE)#
  # Pool sampled and raondomly generated trees tgether:#
  TreePool <- c(Trees, StratPhyloCongruenceResults$rand.trees)#
  # Pool minimum implied gaps for these trees together:#
  MIGs <- c(StratPhyloCongruenceResults$input.tree.results[, "MIG"], StratPhyloCongruenceResults$rand.permutations[, "MIG"])#
#
  # Get tree contradiction matrix:#
  #ContradictionMatrix <- MultiTreeContradiction(trees = TreePool, rescale = TRUE)#
  ##
  RFMatrix <- MultiTreeDistance(trees = TreePool, distance = "RF", scale = TRUE)#
  # Build PCoA from contradiction matrix:#
  #PCoA <- ape::pcoa(as.dist(ContradictionMatrix))#
  # Build PCoA from contradiction matrix:#
  PCoA <- ape::pcoa(asin(sqrt(as.dist(RFMatrix))))#
#
  # Plot as a circular Vornoi to file:#
  pdf(paste("~/Documents/Publications/in prep/Strat congruence - April/ProjectWhalehead/Data/Sample Plots/", DataSets[i], ".pdf", sep = ""))#
  CircularVoronoi(x = PCoA$vectors[, 1], y = PCoA$vectors[, 2], heat = (MIGs - min(MIGs)) / max(MIGs - min(MIGs)), EdgeFactor = 1.1, NColours = 100)#
  dev.off()#
}
for(i in 1:length(DataSets)) {#
}
i
RawAges <- read.table(AgePaths[i], header = TRUE, stringsAsFactors = FALSE)#
  TipAges <- do.call(rbind, lapply(as.list(unique(RawAges[, "Taxon"])), function(x) {y <- unique(apply(RawAges[RawAges[, "Taxon"] == x, c("MaxMa", "MinMa"), drop = FALSE], 1, paste, collapse = "-")); y <- do.call(rbind, strsplit(y, "-")); LAD <- max(as.numeric(y[, 2])); FADs <- as.numeric(y[, 1]); FAD <- min(FADs[FADs >= LAD]); c(FAD, LAD)}))#
  rownames(TipAges) <- unique(RawAges[, "Taxon"])#
  colnames(TipAges) <- c("FAD", "LAD")#
  Trees <- ape::read.tree(MPTPaths[i])#
  if(class(Trees) == "phylo") {#
    Trees <- list(Trees)#
    class(Trees) <- "multiPhylo"#
  }#
  # If tip names and age names do not match:#
  if(length(setdiff(sort(rownames(TipAges)), sort(Trees[[1]]$tip.label))) > 0) {#
    NameMatches <- do.call(rbind, lapply(apply(cbind(sort(rownames(TipAges)), sort(Trees[[1]]$tip.label)), 1, as.list), function(x) unlist(x)))#
    NameMatches <- NameMatches[NameMatches[, 1] != NameMatches [ , 2], , drop = FALSE]#
    for(j in 1:nrow(NameMatches)) {#
      Trees <- lapply(Trees, function(x) {x$tip.label[x$tip.label == NameMatches [j, 2]] <- NameMatches [j, 1]; x})#
      class(Trees) <- "multiPhylo"#
    }#
  }#
  # Get strat congruence results:#
  StratPhyloCongruenceResults <- StratPhyloCongruence(trees = Trees, ages = TipAges, rlen = 0, method = "basic", samp.perm = 1000, rand.perm = 1000, hard = TRUE, randomly.sample.ages = FALSE, fix.topology = FALSE, fix.outgroup = FALSE)#
  # Pool sampled and raondomly generated trees tgether:#
  TreePool <- c(Trees, StratPhyloCongruenceResults$rand.trees)#
  # Pool minimum implied gaps for these trees together:#
  MIGs <- c(StratPhyloCongruenceResults$input.tree.results[, "MIG"], StratPhyloCongruenceResults$rand.permutations[, "MIG"])#
#
  # Get tree contradiction matrix:#
  #ContradictionMatrix <- MultiTreeContradiction(trees = TreePool, rescale = TRUE)#
  # Get tree RF distances:#
  RFMatrix <- MultiTreeDistance(trees = TreePool, distance = "RF", scale = TRUE)#
  # Build PCoA from contradiction matrix:#
  #PCoA <- ape::pcoa(as.dist(asin(sqrt(as.dist(ContradictionMatrix))))#
  # Build PCoA from RF matrix:#
  PCoA <- ape::pcoa(asin(sqrt(as.dist(RFMatrix))))
CircularVoronoi(x = PCoA$vectors[, 1], y = PCoA$vectors[, 2], heat = (MIGs - min(MIGs)) / max(MIGs - min(MIGs)), EdgeFactor = 1.1, NColours = 100)
